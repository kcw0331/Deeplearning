{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "4_4_overfitting_and_underfitting(김창우).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcw0331/Deeplearning/blob/main/4_4_overfitting_and_underfitting(%EA%B9%80%EC%B0%BD%EC%9A%B0).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNMEcLmDispR",
        "outputId": "607ec49a-61bc-4326-8748-029baab407fd"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Mar 30 02:58:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGfnjHM7x_oW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f7516e30-6611-4995-df40-bb4f2ec867d4"
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR9FcKYnx_ob"
      },
      "source": [
        "# Overfitting and underfitting\n",
        "\n",
        "This notebook contains the code samples found in Chapter 3, Section 6 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        "In all the examples we saw in the previous chapter -- movie review sentiment prediction, topic classification, and house price regression -- \n",
        "we could notice that the performance of our model on the held-out validation data would always peak after a few epochs and would then start \n",
        "degrading, i.e. our model would quickly start to _overfit_ to the training data. Overfitting happens in every single machine learning \n",
        "problem. Learning how to deal with overfitting is essential to mastering machine learning.\n",
        "\n",
        "The fundamental issue in machine learning is the tension between optimization and generalization. \"Optimization\" refers to the process of \n",
        "adjusting a model to get the best performance possible on the training data (the \"learning\" in \"machine learning\"), while \"generalization\" \n",
        "refers to how well the trained model would perform on data it has never seen before. The goal of the game is to get good generalization, of \n",
        "course, but you do not control generalization; you can only adjust the model based on its training data.\n",
        "\n",
        "At the beginning of training, optimization and generalization are correlated: the lower your loss on training data, the lower your loss on \n",
        "test data. While this is happening, your model is said to be _under-fit_: there is still progress to be made; the network hasn't yet \n",
        "modeled all relevant patterns in the training data. But after a certain number of iterations on the training data, generalization stops \n",
        "improving, validation metrics stall then start degrading: the model is then starting to over-fit, i.e. is it starting to learn patterns \n",
        "that are specific to the training data but that are misleading or irrelevant when it comes to new data.\n",
        "\n",
        "To prevent a model from learning misleading or irrelevant patterns found in the training data, _the best solution is of course to get \n",
        "more training data_. A model trained on more data will naturally generalize better. When that is no longer possible, the next best solution \n",
        "is to modulate the quantity of information that your model is allowed to store, or to add constraints on what information it is allowed to \n",
        "store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most \n",
        "prominent patterns, which have a better chance of generalizing well.\n",
        "\n",
        "The processing of fighting overfitting in this way is called _regularization_. Let's review some of the most common regularization \n",
        "techniques, and let's apply them in practice to improve our movie classification model from  the previous chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaW6_tYyjWFZ"
      },
      "source": [
        "- The fundamental issue in machine learning is the tension between optimization and generalization. \"Optimization\" refers to the process of adjusting a model to get the best performance possible on the training data (the \"learning\" in \"machine learning\"), while \"generalization\" refers to how well the trained model would perform on data it has never seen before.\n",
        "\n",
        "- 여기는 overfitting과 underfitting에 대해서 소개하고 있다.\n",
        "-  Training data를 통해서 최적화 시키는거랑 이벨류에잇 셋에서 제너럴라이제이션이 얼마나 잘 되는지he fundamental issue in machine learning is the tension between optimization and generalization. \"Optimization\" refers to the process of adjusting a model to get the best performance possible on the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "496SQPitx_oc"
      },
      "source": [
        "Note: in this notebook we will be using the IMDB test set as our validation set. It doesn't matter in this context.\n",
        "\n",
        "Let's prepare the data using the code from Chapter 3, Section 5:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCxL51vlkQI1"
      },
      "source": [
        "- 우리는 데이터를 사용해서 과적합이 어떠한 상황인지랑 레귤렐리제이션의 기법들이 어떤것들이 있는지에 대해 예제를 진행하신다고 말씀하심. 그리고 여기에서 사용하는 것은 Chapter3의 Section 5에 있는 imovie에 대한 데이터이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulR00miPx_oc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60afca43-274b-4949-a2f8-9e765c21a513"
      },
      "source": [
        "#그래서 여기에서는 imdb에 대한 데이터를 불러오고 있다.\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)\n",
        "# Our vectorized labels\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-49z9fRx_oc"
      },
      "source": [
        "# Fighting overfitting\n",
        "\n",
        "## Reducing the network's size\n",
        "\n",
        "\n",
        "The simplest way to prevent overfitting is to reduce the size of the model, i.e. the number of learnable parameters in the model (which is \n",
        "determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is \n",
        "often referred to as the model's \"capacity\". Intuitively, a model with more parameters will have more \"memorization capacity\" and therefore \n",
        "will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any \n",
        "generalization power. For instance, a model with 500,000 binary parameters could easily be made to learn the class of every digits in the \n",
        "MNIST training set: we would only need 10 binary parameters for each of the 50,000 digits. Such a model would be useless for classifying \n",
        "new digit samples. Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge \n",
        "is generalization, not fitting.\n",
        "\n",
        "On the other hand, if the network has limited memorization resources, it will not be able to learn this mapping as easily, and thus, in \n",
        "order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets \n",
        "-- precisely the type of representations that we are interested in. At the same time, keep in mind that you should be using models that have \n",
        "enough parameters that they won't be underfitting: your model shouldn't be starved for memorization resources. There is a compromise to be \n",
        "found between \"too much capacity\" and \"not enough capacity\".\n",
        "\n",
        "Unfortunately, there is no magical formula to determine what the right number of layers is, or what the right size for each layer is. You \n",
        "will have to evaluate an array of different architectures (on your validation set, not on your test set, of course) in order to find the \n",
        "right model size for your data. The general workflow to find an appropriate model size is to start with relatively few layers and \n",
        "parameters, and start increasing the size of the layers or adding new layers until you see diminishing returns with regard to the \n",
        "validation loss.\n",
        "\n",
        "Let's try this on our movie review classification network. Our original network was as such:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkNvexTlk5-V"
      },
      "source": [
        "- overfitting과 싸우는 방법 중 하나가 network를 단순하게 만들어 주는 방법이 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd2sHC_yx_od"
      },
      "source": [
        "#이 코드는 기존에 있던 모형에 대한 것이다.\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "original_model = models.Sequential()  #Sequntial모델로 정의를 하고\n",
        "original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))  #Dense layers를 16짜리를 하나 달고,\n",
        "original_model.add(layers.Dense(16, activation='relu'))  #그 다음 Dense layers를 16짜리를 하나 더 달고,\n",
        "original_model.add(layers.Dense(1, activation='sigmoid')) #그리고 여기에서 sigmoid함수를 사용해서 Dense layers를 1개 달아 준 다음에\n",
        "\n",
        "original_model.compile(optimizer='rmsprop',   #그리고 여기에서 컴파일을 해서 초기모형을 달았다.\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgSBEIsIx_od"
      },
      "source": [
        "Now let's try to replace it with this smaller network:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rNx3epGXGtp"
      },
      "source": [
        "# 사이즈를 작게 해주는 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqfiG3Uvx_oe"
      },
      "source": [
        "#위에서는 Dense layer를 16으로 잡았는데, 여기에서는 Dense layer를 4로 잡아서 단순화 시켰다.\n",
        "smaller_model = models.Sequential()\n",
        "smaller_model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))  #여기에서는 위에서 16으로 했던것을 4로 단다음에\n",
        "smaller_model.add(layers.Dense(4, activation='relu')) #그리고 여기에도 4로 달아주고\n",
        "smaller_model.add(layers.Dense(1, activation='sigmoid')) #여기에서는 1로 달어줘서 이전의 코드보다 훨씬 작게 해주었다.\n",
        "\n",
        "smaller_model.compile(optimizer='rmsprop',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaJn95Q-m0ee",
        "outputId": "0250f030-445f-40e3-f339-f76f5d7c2d7f"
      },
      "source": [
        "#여기는 교수님이 학습을 하기전에 original_model에 대해서 summary를 본다고 하심.\n",
        "original_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,305\n",
            "Trainable params: 160,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0rKdl7PnG-k",
        "outputId": "fddd6b4d-90fa-4ec0-a6e7-ed1274cb1198"
      },
      "source": [
        "#그리고 smaller_model에 대해 summary를 해주심.\n",
        "smaller_model.summary()\n",
        "\n",
        "#그래서 여기에서 보면 Shape이 16에서 4로 줄어든 것을 볼 수 있다. 그리고 Trainable params가 160,305에서 40,029로 줄어든 것을 볼 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 4)                 40004     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 40,029\n",
            "Trainable params: 40,029\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkgJcrvdx_oe"
      },
      "source": [
        "\n",
        "Here's a comparison of the validation losses of the original network and the smaller network. The dots are the validation loss values of \n",
        "the smaller network, and the crosses are the initial network (remember: a lower validation loss signals a better model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhgWG5Bxx_oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc95a24-d04d-4a9a-8226-25de07359c92"
      },
      "source": [
        "#여기에서는 original model을 train시켜본다.\n",
        "original_hist = original_model.fit(x_train, y_train,\n",
        "                                   epochs=20,  #epochs은 20으로 하고 \n",
        "                                   batch_size=512, #batch_size는 512로 해준다.\n",
        "                                   validation_data=(x_test, y_test))\n",
        "#그래서 코드를 돌려보면 train acc는 계속해서 숫자가 올라가는 것을 볼 수 있지만, val_acc는 어느 정도 올라가다가 정체되는 것을 볼 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 4s 67ms/step - loss: 0.5486 - acc: 0.7399 - val_loss: 0.3372 - val_acc: 0.8814\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.2722 - acc: 0.9088 - val_loss: 0.3254 - val_acc: 0.8651\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.2023 - acc: 0.9289 - val_loss: 0.2805 - val_acc: 0.8880\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.1627 - acc: 0.9442 - val_loss: 0.3267 - val_acc: 0.8722\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.1416 - acc: 0.9531 - val_loss: 0.3177 - val_acc: 0.8776\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.1223 - acc: 0.9582 - val_loss: 0.3361 - val_acc: 0.8758\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.1116 - acc: 0.9618 - val_loss: 0.3590 - val_acc: 0.8722\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0931 - acc: 0.9713 - val_loss: 0.3818 - val_acc: 0.8696\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0840 - acc: 0.9724 - val_loss: 0.4172 - val_acc: 0.8636\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0781 - acc: 0.9760 - val_loss: 0.4459 - val_acc: 0.8618\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0667 - acc: 0.9801 - val_loss: 0.4612 - val_acc: 0.8618\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0586 - acc: 0.9835 - val_loss: 0.5222 - val_acc: 0.8549\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0525 - acc: 0.9848 - val_loss: 0.5447 - val_acc: 0.8550\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0478 - acc: 0.9868 - val_loss: 0.5586 - val_acc: 0.8550\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0453 - acc: 0.9867 - val_loss: 0.6293 - val_acc: 0.8484\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0375 - acc: 0.9896 - val_loss: 0.6211 - val_acc: 0.8528\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0340 - acc: 0.9913 - val_loss: 0.6556 - val_acc: 0.8511\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0305 - acc: 0.9926 - val_loss: 0.6922 - val_acc: 0.8502\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0251 - acc: 0.9939 - val_loss: 0.7255 - val_acc: 0.8487\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 2s 43ms/step - loss: 0.0213 - acc: 0.9952 - val_loss: 0.7651 - val_acc: 0.8463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VJ08JXEx_oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3e6a1f-2db5-4a41-f925-3c940dbf003f"
      },
      "source": [
        "#마찬가지로 여기에서는 samller model에 대해서 돌려주고 있다.\n",
        "smaller_model_hist = smaller_model.fit(x_train, y_train,\n",
        "                                       epochs=20,\n",
        "                                       batch_size=512,\n",
        "                                       validation_data=(x_test, y_test))\n",
        "#그래서 코드를 돌려보면 train acc는 계속해서 숫자가 올라가는 것을 볼 수 있지만, val_acc는 어느 정도 올라가다가 정체되는 것을 볼 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 3s 42ms/step - loss: 0.6713 - acc: 0.5839 - val_loss: 0.6179 - val_acc: 0.8069\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.5862 - acc: 0.7761 - val_loss: 0.5530 - val_acc: 0.7996\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.5189 - acc: 0.8313 - val_loss: 0.5112 - val_acc: 0.8455\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.4721 - acc: 0.8694 - val_loss: 0.4821 - val_acc: 0.8526\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.4412 - acc: 0.8907 - val_loss: 0.4624 - val_acc: 0.8610\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.4070 - acc: 0.9095 - val_loss: 0.4488 - val_acc: 0.8666\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 2s 34ms/step - loss: 0.3833 - acc: 0.9217 - val_loss: 0.4379 - val_acc: 0.8727\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.3654 - acc: 0.9321 - val_loss: 0.4291 - val_acc: 0.8779\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.3488 - acc: 0.9423 - val_loss: 0.4218 - val_acc: 0.8815\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.3297 - acc: 0.9482 - val_loss: 0.4191 - val_acc: 0.8790\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.3171 - acc: 0.9510 - val_loss: 0.4260 - val_acc: 0.8724\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.3003 - acc: 0.9575 - val_loss: 0.4163 - val_acc: 0.8776\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 2s 34ms/step - loss: 0.2891 - acc: 0.9599 - val_loss: 0.4177 - val_acc: 0.8756\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.2745 - acc: 0.9643 - val_loss: 0.4289 - val_acc: 0.8715\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.2620 - acc: 0.9664 - val_loss: 0.4205 - val_acc: 0.8742\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.2516 - acc: 0.9707 - val_loss: 0.4119 - val_acc: 0.8723\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.2435 - acc: 0.9698 - val_loss: 0.4226 - val_acc: 0.8712\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.2323 - acc: 0.9729 - val_loss: 0.4371 - val_acc: 0.8688\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.2220 - acc: 0.9748 - val_loss: 0.4366 - val_acc: 0.8676\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.2117 - acc: 0.9776 - val_loss: 0.4474 - val_acc: 0.8672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfvzHdqsx_of"
      },
      "source": [
        "#train 시켜준것을 history에 저장을 해서 original_val_loss랑 samller_model_val_loss에 대한 그림을 그려준다.\n",
        "epochs = range(1, 21)\n",
        "original_val_loss = original_hist.history['val_loss']\n",
        "smaller_model_val_loss = smaller_model_hist.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97KNkYJzx_of",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "d047b4b2-97f0-494d-fcf6-7f985755b5a1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# b+ is for \"blue cross\"\n",
        "plt.plot(epochs, original_val_loss, 'b+', label='Original model') #original은 파란색 +로 되어있고,\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, smaller_model_val_loss, 'bo', label='Smaller model') #smaller는 파랜색 o로 되어 있는 것을 볼 수 있다.\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "#그림에서 보면 original model이 처음에는 최적화 되었다가 점차 overfitting되어가는 것을 볼 수 있다.\n",
        "#그리고 smaller는 조금사용해서 과적합이 조금더 천천히 일어나는 것을 볼 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxfB9CZYx_of"
      },
      "source": [
        "\n",
        "As you can see, the smaller network starts overfitting later than the reference one (after 6 epochs rather than 4) and its performance \n",
        "degrades much more slowly once it starts overfitting.\n",
        "\n",
        "Now, for kicks, let's add to this benchmark a network that has much more capacity, far more than the problem would warrant:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwd2bibGXnkm"
      },
      "source": [
        "# 사이즈를 크게 한 경우"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wwVDbUYx_of"
      },
      "source": [
        "#smaller로 줄이게 되면 위에 보이는 그림 처럼 과적합이 천천히 되는 것을 볼 수 있었는데, 우리는 여기에서 bigger로 하게되면 어떠하게 되는지에 대해 보게된다. \n",
        "bigger_model = models.Sequential()\n",
        "bigger_model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))  #여기에서는 512개를 달아가지고 \n",
        "bigger_model.add(layers.Dense(512, activation='relu')) #여기도 512를 달아주고 \n",
        "bigger_model.add(layers.Dense(1, activation='sigmoid')) #마지막에는 1을 달아준다. 그리고 모형을 만들어 준다.\n",
        "\n",
        "bigger_model.compile(optimizer='rmsprop',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuWwzuy2qB9v",
        "outputId": "a01e0a29-93c3-4511-c392-40dd84954723"
      },
      "source": [
        "#여기에서는 교수님이 bigger model에 대한 summary를 보셨다.\n",
        "bigger_model.summary()\n",
        "\n",
        "#코드를 돌려본 결과 Trainable params가 5,383,681개의 parameter를 수행하는 것을 볼 수 있다.\n",
        "#즉, 크기를 크게 한 결과 Trainable params의 숫자도 커진것을 확인 할 수 있습니다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 512)               5120512   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 5,383,681\n",
            "Trainable params: 5,383,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2jsHQ1cx_of",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9fc73e-1ecd-499a-83ec-664e0daa843d"
      },
      "source": [
        "#여기에서는 bigger model로 train을 진행해준다.\n",
        "bigger_model_hist = bigger_model.fit(x_train, y_train,\n",
        "                                     epochs=20,\n",
        "                                     batch_size=512,\n",
        "                                     validation_data=(x_test, y_test))\n",
        "#그리고 parameter를 많이 사용하다보니깐 train acc가 점점 높아지는 것을 알 수 있다.\n",
        "#그래서 너무 많은 parameter를 사용하다 보니 train acc가 1.0000으로 되서 과적합이 일어나는 것을 볼 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 17s 336ms/step - loss: 0.6075 - acc: 0.7051 - val_loss: 0.2863 - val_acc: 0.8830\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 16s 321ms/step - loss: 0.2272 - acc: 0.9101 - val_loss: 0.3402 - val_acc: 0.8654\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 16s 321ms/step - loss: 0.1250 - acc: 0.9569 - val_loss: 0.6596 - val_acc: 0.7865\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 16s 323ms/step - loss: 0.0745 - acc: 0.9758 - val_loss: 1.1603 - val_acc: 0.7501\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 16s 321ms/step - loss: 0.1262 - acc: 0.9658 - val_loss: 0.5559 - val_acc: 0.8332\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 16s 321ms/step - loss: 0.0116 - acc: 0.9995 - val_loss: 0.6637 - val_acc: 0.8804\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 16s 321ms/step - loss: 0.0247 - acc: 0.9971 - val_loss: 0.5634 - val_acc: 0.8758\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 16s 322ms/step - loss: 9.8869e-04 - acc: 1.0000 - val_loss: 0.7650 - val_acc: 0.8776\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 16s 322ms/step - loss: 1.6776e-04 - acc: 0.9999 - val_loss: 0.9484 - val_acc: 0.8774\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 16s 322ms/step - loss: 0.1400 - acc: 0.9891 - val_loss: 0.7752 - val_acc: 0.8734\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 16s 324ms/step - loss: 1.1209e-04 - acc: 1.0000 - val_loss: 0.8737 - val_acc: 0.8753\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 16s 322ms/step - loss: 2.4766e-05 - acc: 1.0000 - val_loss: 1.0062 - val_acc: 0.8762\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 16s 321ms/step - loss: 4.1208e-06 - acc: 1.0000 - val_loss: 1.1753 - val_acc: 0.8760\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 16s 323ms/step - loss: 0.0946 - acc: 0.9941 - val_loss: 0.9329 - val_acc: 0.8760\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 16s 323ms/step - loss: 7.5363e-06 - acc: 1.0000 - val_loss: 0.9603 - val_acc: 0.8757\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 16s 323ms/step - loss: 4.1071e-06 - acc: 1.0000 - val_loss: 0.9986 - val_acc: 0.8759\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 16s 324ms/step - loss: 2.3054e-06 - acc: 1.0000 - val_loss: 1.1009 - val_acc: 0.8769\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 16s 323ms/step - loss: 7.6850e-07 - acc: 1.0000 - val_loss: 1.2419 - val_acc: 0.8774\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 16s 323ms/step - loss: 1.3700e-07 - acc: 1.0000 - val_loss: 1.3841 - val_acc: 0.8776\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 16s 323ms/step - loss: 3.8560e-08 - acc: 1.0000 - val_loss: 1.4657 - val_acc: 0.8774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlCfbURBx_og"
      },
      "source": [
        "Here's how the bigger network fares compared to the reference one. The dots are the validation loss values of the bigger network, and the \n",
        "crosses are the initial network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K-ciRNBx_og",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "b49f130a-f18d-440b-b789-a8ab44e80181"
      },
      "source": [
        "#여기에서는 original과 bigger를 비교를 해서 그림을 그려준다.(original_val_loss와 bigger_model_val_loss를 사용함.)\n",
        "bigger_model_val_loss = bigger_model_hist.history['val_loss']\n",
        "\n",
        "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
        "plt.plot(epochs, bigger_model_val_loss, 'bo', label='Bigger model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "#그림에서 보았을 때, bigger model은 어느정도 해까지 근접했다가 되게 빠르게 안좋아지는 것을 볼 수 있다. 그래서 overfitting이 더 빠르게 일어나는 것을 알 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV9b3/8deHiOKCC4heBElAQXYUAkqxrhWtC7i0Vcy1ohbq1kttf71q8Upaf7a1eqt1qxcrojXVq1Kt99ZWahWsii3BogguoAYbtBWoFmmkEvjcP2YSQzhbcs6cbd7Px+M8cs7MnJlPTk7mM/P9fucz5u6IiEh8dSl0ACIiUlhKBCIiMadEICISc0oEIiIxp0QgIhJzOxQ6gI7ae++9vaqqqtBhiIiUlCVLlqxz916J5pVcIqiqqqK+vr7QYYiIlBQzW51snpqGRERiTolARCTmlAhERGKu5PoIEtm8eTONjY1s2rSp0KFIhrp160bfvn3p2rVroUMRib2ySASNjY10796dqqoqzKzQ4Uga7s769etpbGykf//+hQ5HJPbKomlo06ZN9OzZU0mgRJgZPXv21BmcSIbq6qCqCrp0CX7W1eV2/WVxRgAoCZQY/b1EMlNXB9OnQ1NT8Hr16uA1QE1NbrZRFmcEIiLlaubMT5NAi6amYHquKBHkSGNjI5MnT2bgwIEccMABzJgxg08++SThsu+++y5f+MIX0q7zxBNP5MMPP+xUPLW1tdxwww2dem+m5s6dy6WXXpr1MiKS3DvvdGx6Z8Q6EdTW5mY97s7pp5/OqaeeysqVK3njjTfYuHEjMxOk7ObmZvbbbz8efvjhtOt9/PHH2XPPPXMTpIiUpH79Oja9M2KdCL7zndys56mnnqJbt26cd955AFRUVHDjjTcyZ84cmpqamDt3LpMmTeKYY47h2GOPpaGhgeHDhwPQ1NTEl770JYYOHcppp53GoYce2lpCo6qqinXr1tHQ0MCQIUOYNm0aw4YNY+LEiXz88ccA3HnnnYwdO5ZRo0Zxxhln0NT+HLKdqVOnctFFF3HYYYcxYMAAFixYwPnnn8+QIUOYOnVq63L3338/I0aMYPjw4Vx++eWt0++++24GDRrEuHHjeO6551qnr127ljPOOIOxY8cyduzYbeaJSOddey3sssu203bZJZieK7FOBLmyfPlyxowZs8203XffnX79+rFq1SoAXnzxRR5++GEWLly4zXK33347e+21FytWrOCaa65hyZIlCbexcuVKLrnkEpYvX86ee+7JvHnzADj99NNZvHgxL730EkOGDOGuu+5KG+8HH3zAokWLuPHGG5k0aRKXXXYZy5cvZ9myZSxdupR3332Xyy+/nKeeeoqlS5eyePFiHn30Ud577z1mzZrFc889x7PPPsuKFSta1zljxgwuu+wyFi9ezLx58/jKV77Soc9QRBKrqYHZs6GyEsyCn7Nn566jGMpo1FCmamu3PRNoGbwya1bumooSOe644+jRo8d205999llmzJgBwPDhwxk5cmTC9/fv35+DDz4YgDFjxtDQ0ADAK6+8wlVXXcWHH37Ixo0bOf7449PGcsopp2BmjBgxgn333ZcRI0YAMGzYMBoaGli9ejVHHXUUvXoFhQpramp45plnALaZfuaZZ/LGG28A8OSTT26TGDZs2MDGjRvTxiIi6dXU5HbH314sE0HLDt8M3LNf59ChQ7dr89+wYQPvvPMOBx54IC+++CK77rprVtvYaaedWp9XVFS0Ng1NnTqVRx99lFGjRjF37lwWLFiQ8bq6dOmyzXq7dOlCc3Nzp6723bp1Ky+88ALdunXr8HtFpLDUNJQDxx57LE1NTdx7770AbNmyhW9+85tMnTqVXdo37rUzYcIEHnzwQQBWrFjBsmXLOrTtjz76iN69e7N582bqcnSVybhx41i4cCHr1q1jy5Yt3H///Rx55JEceuihLFy4kPXr17N582Yeeuih1vdMnDiRW265pfX10qVLcxKLiEQv1olg1qzcrMfMeOSRR3jooYcYOHAggwYNolu3bnzve99L+96LL76YtWvXMnToUK666iqGDRvGHnvskfG2r7nmGg499FAmTJjA4MGDs/k1WvXu3Zsf/OAHHH300YwaNYoxY8YwefJkevfuTW1tLePHj2fChAkMGTKk9T0333wz9fX1jBw5kqFDh3LHHXfkJBYRiZ55LtpGEq3YbA5wMvC+uw9PsdxYYBFwlrunHVNZXV3t7W9M8+qrr26zUyolW7ZsYfPmzXTr1o0333yTz33uc7z++uvsuOOOhQ4tcqX8dxMpNWa2xN2rE82Lso9gLnArcG+yBcysArgOmB9hHEWtqamJo48+ms2bN+Pu3H777bFIAiJSPCJLBO7+jJlVpVnsa8A8YGxUcRS77t2769abIlJQBesjMLM+wGnATzJYdrqZ1ZtZ/dq1a6MPTkQkh6KuHpqtQnYW3wRc7u5b0y3o7rPdvdrdq1vGsIuIlIKW6qGrVwfD1VuqhxZTMihkIqgGHjCzBuALwO1mdmoB4xERybl8VA/NVsEuKHP31ltTmdlc4H/d/dFCxSMiEoV8VA/NVmRnBGZ2P8Gw0IPMrNHMLjCzC83swqi2WUgVFRUcfPDBjBo1itGjR/P8888DmZecLma77bZbTpYRiaN8VA/NVmSJwN2nuHtvd+/q7n3d/S53v8Pdt7vSyN2nZnINQa5E0XGz8847s3TpUl566SW+//3vc+WVVwJkXHI6G83NzZGuX0Q6Lx/VQ7MVuyuL89Fxs2HDBvbaay+AjEtO33XXXa3lnadNm9Z6M5dk5Z1ra2s555xzmDBhAuecc84221+wYAFHHnkkkydPZsCAAVxxxRXU1dUxbtw4RowYwZtvvtka2zHHHMPIkSM59thjeSc8V3377bcZP348I0aM4Kqrrtpm3ddffz1jx45l5MiRzMrVpdkiZSwf1UOz5u4l9RgzZoy3t2LFiu2mJVNZ6R6kgG0flZUZryKhLl26+KhRo/yggw7y3Xff3evr693d/e233/Zhw4a5u/v111/v06dPd3f3ZcuWeUVFhS9evNjXrFnjlZWVvn79ev/kk0/88MMP90suucTd3adMmeK///3v3d199erVPnjwYHd3nzVrlo8ePdqbmpq2i+Xpp5/2PfbYw999913ftGmT77fffn711Ve7u/tNN93kM2bMcHf3k08+2efOnevu7nfddZdPnjzZ3d1POeUUv+eee9zd/dZbb/Vdd93V3d2feOIJnzZtmm/dutW3bNniJ510ki9cuNDdvXWZjujI301EsgPUe5L9auzOCKLquGlpGnrttdf4zW9+w5e//GW8XfmOZ599lrPOOgvYtuT0H//4R4488kh69OhB165d+eIXv9j6nieffJJLL72Ugw8+mEmTJm1T3nnSpEnsvPPOCeMZO3YsvXv3ZqedduKAAw5g4sSJAIwYMaK1hPWiRYs4++yzATjnnHN49tlnAXjuueeYMmVK6/QW8+fPZ/78+RxyyCGMHj2a1157jZUrV2b1uYlI4cWuDHW/fkFzUKLpuTJ+/HjWrVtHLi5+S1XeOVVp6/blpduWns6kT8FabtTQhrtz5ZVX8tWvfjWT0EWkRMTujCAfHTevvfYaW7ZsoWfPnttMT1ZyeuzYsSxcuJAPPviA5ubm1ruPQbTlnT/zmc/wwAMPAFBXV8dnP/vZ1jjbTm9x/PHHM2fOnNYzkjVr1vD+++/nLB4RKYzYnRG0dNDMnBk0B/XrFySBbDtuPv7449Y7iLk799xzDxUVFdssc/HFF3PuuecydOhQBg8e3Fpyuk+fPnz7299m3Lhx9OjRg8GDB7eWor755pu55JJLGDlyJM3NzRxxxBE5K/F8yy23cN5553H99dfTq1cv7r77bgB+/OMfc/bZZ3PdddcxefLk1uUnTpzIq6++yvjx44FgyOh9993HPvvsk5N4RKQwIitDHZVSLkOdquT0xo0b2W233Whubua0007j/PPP57TTTit0yJEqlb+bSDkoVBlqaSdVyena2lqefPJJNm3axMSJEzn1VFXbEJH8UCLIo1Qlp2+44YY8RyMiEiibzuJSa+KKO/29RIpHWSSCbt26sX79eu1cSoS7s379+oRDYkUk/8qiaahv3740NjbmZNy+5Ee3bt3o27dvocMQEcokEXTt2pX+/funX1BERLZTFk1DIiLSeUoEIiIxp0QgIhJzSgQiIjGnRCAiEnNKBCIiaURxe9tiUhbDR0VEotJye9umpuB1y+1tochuN5kFnRGIiKQwc+anSaBFU1MwvVwoEYiIpBDV7W2LiRKBiEgKyW5jm8vb2xaaEoGISAr5uL1toUWWCMxsjpm9b2avJJlfY2Yvm9kyM3vezEZFFYuISGfV1MDs2VBZCWbBz9mzy6ejGKIdNTQXuBW4N8n8t4Ej3f0DM/s8MBs4NMJ4REQ6paamvHb87UWWCNz9GTOrSjH/+TYvXwBUk1hEpACKpY/gAuDXhQ5CRCSOCn5BmZkdTZAIDk+xzHRgOkC/cuqqFxEpAgU9IzCzkcBPgcnuvj7Zcu4+292r3b26V69e+QtQpEyUe4kEyU7BzgjMrB/wC+Acd3+jUHGIlLs4lEiQ7EQ5fPR+YBFwkJk1mtkFZnahmV0YLnI10BO43cyWmll9VLGUOh3NSTbiUCJBsmPuXugYOqS6utrr6+OTM9ofzUFwMUu5jWOW6HTpAon+zc1g69b8xyOFYWZL3L060bxiGTUkSehoTrIVhxIJkh0lgiIXh4JXEq04lEiQ7CgRFDkdzUm24lAiQbKjRFDkdDQnuVBTAw0NQZ9AQ4OSgGxLiaDI6WhORKJW8CuLJb1yL3glIoWlMwIRkZhTIhARiTklAhEpe7o6PzX1EYhIWVOtpfR0RiAiZU1X56enRCAiZU1X56enRCAiZU1X56enRCAiZU1X56enRCAiZU1X56eXdtSQmU0Alrr7P8zsX4HRwI/dfXXk0YmI5ICuzk8tkzOCnwBNZjYK+CbwJnBvpFGJiEjeZJIImj24jdlk4FZ3vw3oHm1YIiKSL5lcUPaRmV0J/CtwhJl1AbpGG5aIiORLJmcEZwL/BC5w978AfYHrI41KRETyJqMzAoLO4S1mNggYDNwfbVgiIpIvmZwRPAPsZGZ9gPnAOcDcKIMSEZH8ySQRmLs3AacDt7v7F4Hh0YYlIiL5klEiMLPxQA3wqw68T0RESkAmO/SvA1cCj7j7cjMbADwdbVgi0pbq6UuU0iYCd1/o7pOA28xsN3d/y93/Ld37zGyOmb1vZq8kmW9mdrOZrTKzl81sdCfiFyl7LfX0V68G90/r6ccpGSgRRittIjCzEWb2J2A5sMLMlpjZsAzWPRc4IcX8zwMDw8d0giuYRaSduNfTVyKMXiZNQ/8FfMPdK929H0GZiTvTvcndnwH+lmKRycC9HngB2NPMemcStEicxL2eftwTYT5kkgh2dffWPgF3XwDsmoNt9wH+3OZ1YzhtO2Y23czqzax+7dq1Odi0SOmIez39uCfCfMgkEbxlZv9hZlXh4yrgragDa8vdZ7t7tbtX9+rVK5+bFim4uNfTj3sizIdMEsH5QC/gF+GjVzgtW2uA/du87htOE5E24l5PP+6JMB/Slphw9w+AtKOEOuEx4FIzewA4FPi7u78XwXZESl6c6+m3/N4zZwbNQf36BUkgrp9HFJImAjP7H8CTzQ+HlCZlZvcDRwF7m1kjMIuwaqm73wE8DpwIrAKagPM6GLuIlIi6uux25HFOhPmQ6ozghmxW7O5T0sx34JJstiEixa9l+GfLyJ+W4Z+gnXuxSNpHEF5IlvSRzyBFsqULkgpHwz+LXyZlqEVKmo5IC0vDP4ufisdJ2dMRafayOaPS8M/ip0QgZU9HpNnJtsSDhn8Wv0xqDQ0yszvNbL6ZPdXyyEdwIrmgI9LsZHtGFffrIEqBBYN3Uixg9hJwB7AE2NIy3d2XRBtaYtXV1V5fX1+ITUuJat9HAMERqXZGmenSJTgTaM8Mtm7NfzzSOWa2xN2rE83LpLO42d1VGVRKli5Iyk6/fkFzUKLpUh4y6SP4HzO72Mx6m1mPlkfkkYnkUE0NNDQER7ANDUoCHaE2/vKXyRnBueHPb7WZ5sCA3IcjIsVGZ1TlL5NaQ/3zEYiIFC+VeChvaROBmXUFLgKOCCctAP7L3TdHGJeIiORJJk1DPyEoFnd7+PqccNpXogpKRETyJ5PO4rHufq67PxU+zgPGRh2YSDlRrSMpZpmcEWwxswPc/U0AMxtAm+sJRCQ11TqSYpfJGcG3gKfNbIGZLQSeIriBvYhkQLWOpNhlMmrod2Y2EDgonPS6u/8z2rBEyodqHUmxS3WHsmPc/SkzO73drAPNDHf/RcSxiZQFXZkrxS7VGcGRBM1ApySY5wQ3sheRNK69NnGtI12ZK8UiaSJw91nh0++6+9tt55mZLjITyZCuzJVil8mooXnA6HbTHgbG5D4ckfKkK3OlmKXqIxgMDAP2aNdPsDvQLerAREQkP1KdERwEnAzsybb9BB8B06IMSkRE8idVH8EvgV+a2Xh3X5THmEREJI8y6SP4k5ldQtBM1Nok5O7nRxaViIjkTSZXFv8M+BfgeGAh0JegeSgtMzvBzF43s1VmdkWC+f3M7Gkz+5OZvWxmJ3YkeBERyV4mieBAd/8P4B/ufg9wEnBoujeZWQVwG/B5YCgwxcyGtlvsKuBBdz8EOItPK5yKiEieZJIIWu478KGZDQf2APbJ4H3jgFXu/pa7fwI8AExut4wTjEIiXO+7GaxXRERyKJM+gtlmthfwH8BjwG7A1Rm8rw/w5zavG9n+TKIWmG9mXwN2BT6XaEVmNh2YDtBP1+WLiORU2jMCd/+pu3/g7gvdfYC77+Pud+Ro+1OAue7eFzgR+JmZbReTu89292p3r+7Vq1eONi2SOd1PQMpZqgvKvpHqje7+ozTrXgPs3+Z133BaWxcAJ4TrW2Rm3YC9gffTrFskb3Q/ASl3qc4IuoePaoJ7FvcJHxeyfcmJRBYDA82sv5ntSNAZ/Fi7Zd4BjgUwsyEEw1PXduQXkHgo5BG57icgxaK2Npr1mrunXsDsGeAkd/8ofN0d+JW7H5HyjcGyJwI3ARXAHHe/1sy+C9S7+2PhKKI7CfodHPh3d5+fap3V1dVeX1+fwa8m5aL9ETkE1Ttnz87PEXmXLpDo38QMtm6NfvsiLcwSfxcze68tcffqhPMySASvAyNbbkZjZjsBL7v7QSnfGBElgvipqkpcz7+yEhoayn/7Ii2iSgSZDB+9F/ijmdWaWS3wB2Bu50IR6bhC3+Hr2muDM5C2dD8ByZfa2iABmAWvW57nspko7RlBsGEbDXw2fPmMu/8pdyF0jM4I4qcYjsjr6nQ/ASm8qM4IUo0a2t3dN5hZD6AhfLTM6+Huf+tcOCIdUwx3+NL9BKScpbqg7OcEZaiXEHTktrDw9YAI4xJppTt8iQRmzUq/TGdk1DRUTNQ0JCKlqrY2uiGg6XRq1FDYL5CUu7+Yg9g6TIlAREpVNm382W+7E30EwH+mmOfAMVlFJSIiRSHp8FF3PzrFQ0lARCQD+Rj+ma1Mh48OJ7inQNs7lN0bYVxJqWlIREpVsTYNpb2gzMxmAbeEj6OBHwKTchqhiEgJKKaj+FzK5MriLxAUhvuLu58HjCK4iYyISKx85zvZvT+q4Z/ZyiQRfOzuW4FmM9udoET0/mneIyIi7RTrGUUmiaDezPYkqBK6BHgRWBRpVCIiRaIUOnuzleo6gtuAn7v7c22mVQG7u/vLeYkuAXUWi0ihFLKzN1ud7Sx+A7jBzBrM7Idmdoi7NxQyCUjh6FaNIuUr1XUEP3b38cCRwHpgjpm9ZmazzGxQ3iIsA6W+E225Mczq1cHRUMutGkvt9xDJtjmnWDt7s9WhWkNmdggwh+BGNRWRRZVCqTUNFfruWrlQDGWgRXKhlJt2spXtdQQ7mNkpZlYH/Bp4HTg9xzGWrXK4322hbwwjItFKmgjM7DgzmwM0AtOAXwEHuPtZ7v7LfAVY6sphJ9qvX8emixSTOIz6yVaqM4IrgeeBIe4+yd1/7u7/yFNcZaMcdqK6VaOUstraoDmopUmo5bkSwadSdRYf4+4/dfcP8hlQuSmHnWhNTdCnUVkZHElVVpZWH4cUD+18i1MmF5RJFsplJ1pTE3QMb90a/Cy1+KU4ZFuiQaN+oqE7lIlI3mQ7aifOo36yldWoIRGRbKiztvhFmgjM7AQze93MVpnZFUmW+ZKZrTCz5Wb28yjjEZH8y7azVokkepE1DZlZBUGZiuMIhqAuBqa4+4o2ywwEHgSOcfcPzGwfd38/1XrVNCRSutQ0VDiFahoaB6xy97fc/RPgAWByu2WmAbe1jExKlwREpLSps7Y4RZkI+gB/bvO6MZzW1iBgkJk9Z2YvmNkJEcYjIgWmUT/FaYci2P5A4CigL/CMmY1w9w/bLmRm04HpAP1K6UosEckp9QtEI8ozgjVseyezvuG0thqBx9x9s7u/TdCnMLD9itx9trtXu3t1r169IgtYRCSOokwEi4GBZtbfzHYEzgIea7fMowRnA5jZ3gRNRW9FGJOIiLQTWSJw92bgUuAJ4FXgQXdfbmbfNbNJ4WJPAOvNbAXwNPAtd18fVUxSOKV+TwaRchbpdQTu/ri7D3L3A9z92nDa1e7+WPjc3f0b7j7U3Ue4+wNRxhNXhd4J68Y25UNt9OVJJSbKXDHcGEc3tikfGsdfulRiIsaK4cY45XBPhnKhI3pJRImgzBXDTrgc7slQLjpT/VMlHsqfEkGZK4adcDnckyHOdGOX8qdEUOaKYSdcLvdkKFU6opd0lAjKXLHshHVjm9zozM47l0f0KvFQnjRqSKSEqHqndJZGDYkIoCN6SUyJQKTI5bKNX/0CkoiahkRKiJp2pLPUNCQiIkkpEYiUELXxSxSUCERKiNr4JQpKBCIiMadEICISc0oEInmkph0pRkoEInnUmeqfIlFTIhDpAB3RSzlSIhDpANXzl3KkK4tFOkBF36RU6cpikSzoiF7K3Q6FDkCk2NXWfrrTz/aIXlcGSzHSGYHESqGP4gu9fZFElAgkVrIdvqkjeilHSgQiHaAjeilHkSYCMzvBzF43s1VmdkWK5c4wMzezhD3aItlQZ69IapENHzWzCuAN4DigEVgMTHH3Fe2W6w78CtgRuNTdU44N1fBRyYaGb0pcFWr46Dhglbu/5e6fAA8AkxMsdw1wHbApwlhERCSJKBNBH+DPbV43htNamdloYH93/1WqFZnZdDOrN7P6tWvX5j5SKRnZNueos1dkewXrLDazLsCPgG+mW9bdZ7t7tbtX9+rVK/rgpGhlO+pH/QIi24syEawB9m/zum84rUV3YDiwwMwagMOAx9RhLCKSX1EmgsXAQDPrb2Y7AmcBj7XMdPe/u/ve7l7l7lXAC8CkdJ3FEj8a9SMSrcgSgbs3A5cCTwCvAg+6+3Iz+66ZTYpqu1LcOrPzrq0NRvq0jPZpea5EIJIbqj4qeaXqnSKFoeqjUjY06kck92KVCDrblFBXB1VV0KVL8LOuLodBxUAu2/jVHCSSe7FqGupMs0JdHUyfDk1Nn07bZReYPRtqajoVRklrW5K5M9S0I1IYahrKwsyZ2yYBCF7PnJn/WIrhaFg3XxcpP2WfCLJtlnjnnY5Nj1I57ITVxi9SfGKRCLIZetivX8emF7POnlGojV+kvJV9IsjWmDEdm55rudwJd/aMQuP4RcpbrBJBZ5ol5s2D++6DysrgdWVl8HrevNzGlox2wiIStVglgs7uPGtqoKEheN7QUFqjhXJdnkFt/CLlZ4dCB1BKCr0T7Mz22w73zMXQTZ2JiJSfWJwR5OqCsELvBAu9fREpT2WfCFouCFu9OjgaXr06eF2Iq4MLvSMv9BmNiBSnsr+yuKoq2Pm3V1n5abt/vuiqWhEplFhfWVxMF4SJiBSjsk8Ehb4gTDdVEZFiV/aJ4NprgyJxbe2ySzA9H3QdgIgUu7JPBDU1QaXQysrgSLyyMr6VQ0VEEonFdQQ1NcWx49eoHREpRmV/RlBM1BwkIsVIiUBEJOaUCEREYk6JQEQk5pQIRERiTolARCTmSq7WkJmtBRJUDyoKewPrCh1ECsUeHxR/jIovO4ovO9nEV+nuvRLNKLlEUMzMrD5ZUadiUOzxQfHHqPiyo/iyE1V8ahoSEYk5JQIRkZhTIsit2YUOII1ijw+KP0bFlx3Fl51I4lMfgYhIzOmMQEQk5pQIRERiTomgg8xsfzN72sxWmNlyM5uRYJmjzOzvZrY0fFyd5xgbzGxZuO3tbvBsgZvNbJWZvWxmo/MY20FtPpelZrbBzL7ebpm8f35mNsfM3jezV9pM62FmvzWzleHPvZK899xwmZVmdm4e47vezF4L/4aPmNmeSd6b8vsQYXy1Zramzd/xxCTvPcHMXg+/j1fkMb7/bhNbg5ktTfLeSD+/ZPuUvH7/3F2PDjyA3sDo8Hl34A1gaLtljgL+t4AxNgB7p5h/IvBrwIDDgD8UKM4K4C8EF7oU9PMDjgBGA6+0mfZD4Irw+RXAdQne1wN4K/y5V/h8rzzFNxHYIXx+XaL4Mvk+RBhfLfD/MvgOvAkMAHYEXmr//xRVfO3m/ydwdSE+v2T7lHx+/3RG0EHu/p67vxg+/wh4FehT2Kg6bDJwrwdeAPY0s94FiONY4E13L/iV4u7+DPC3dpMnA/eEz+8BTk3w1uOB37r739z9A+C3wAn5iM/d57t7c/jyBaBvrrebqSSfXybGAavc/S13/wR4gOBzz6lU8ZmZAV8C7s/1djORYp+St++fEkEWzKwKOAT4Q4LZ483sJTP7tZkNy2tg4MB8M1tiZtMTzO8D/LnN60YKk8zOIvk/XyE/vxb7uvt74fO/APsmWKZYPsvzCc7yEkn3fYjSpWHT1ZwkTRvF8Pl9Fviru69MMj9vn1+7fUrevn9KBJ1kZrsB84Cvu/uGdrNfJGjuGAXcAjya5/AOd/fRwOeBS8zsiDxvPy0z2xGYBDyUYHahP7/teHAeXpRjrc1sJtAM1CVZpFDfh58ABwAHA+8RNL8UoymkPhvIy7IWcQMAAAOnSURBVOeXap8S9fdPiaATzKwrwR+szt1/0X6+u29w943h88eBrma2d77ic/c14c/3gUcITr/bWgPs3+Z133BaPn0eeNHd/9p+RqE/vzb+2tJkFv58P8EyBf0szWwqcDJQE+4stpPB9yES7v5Xd9/i7luBO5Nst9Cf3w7A6cB/J1smH59fkn1K3r5/SgQdFLYn3gW86u4/SrLMv4TLYWbjCD7n9XmKb1cz697ynKBD8ZV2iz0GfDkcPXQY8Pc2p6D5kvQorJCfXzuPAS2jMM4FfplgmSeAiWa2V9j0MTGcFjkzOwH4d2CSuzclWSaT70NU8bXtdzotyXYXAwPNrH94lngWweeeL58DXnP3xkQz8/H5pdin5O/7F1VPeLk+gMMJTtFeBpaGjxOBC4ELw2UuBZYTjIB4AfhMHuMbEG73pTCGmeH0tvEZcBvBaI1lQHWeP8NdCXbse7SZVtDPjyApvQdsJmhnvQDoCfwOWAk8CfQIl60GftrmvecDq8LHeXmMbxVB+3DL9/COcNn9gMdTfR/yFN/Pwu/XywQ7td7t4wtfn0gwUubNfMYXTp/b8r1rs2xeP78U+5S8ff9UYkJEJObUNCQiEnNKBCIiMadEICISc0oEIiIxp0QgIhJzSgQiITPbYttWRs1ZJUwzq2pb+VKkmOxQ6ABEisjH7n5woYMQyTedEYikEdaj/2FYk/6PZnZgOL3KzJ4Ki6r9zsz6hdP3teD+AC+Fj8+Eq6owszvDmvPzzWzncPl/C2vRv2xmDxTo15QYUyIQ+dTO7ZqGzmwz7+/uPgK4FbgpnHYLcI+7jyQo+HZzOP1mYKEHRfNGE1yRCjAQuM3dhwEfAmeE068ADgnXc2FUv5xIMrqyWCRkZhvdfbcE0xuAY9z9rbA42F/cvaeZrSMom7A5nP6eu+9tZmuBvu7+zzbrqCKoGz8wfH050NXd/7+Z/QbYSFBl9VEPC+6J5IvOCEQy40med8Q/2zzfwqd9dCcR1H4aDSwOK2KK5I0SgUhmzmzzc1H4/HmCapkANcDvw+e/Ay4CMLMKM9sj2UrNrAuwv7s/DVwO7AFsd1YiEiUdeYh8amfb9gbmv3H3liGke5nZywRH9VPCaV8D7jazbwFrgfPC6TOA2WZ2AcGR/0UElS8TqQDuC5OFATe7+4c5+41EMqA+ApE0wj6CandfV+hYRKKgpiERkZjTGYGISMzpjEBEJOaUCEREYk6JQEQk5pQIRERiTolARCTm/g9YAyH/tshnlwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r38lCtbx_og"
      },
      "source": [
        "\n",
        "The bigger network starts overfitting almost right away, after just one epoch, and overfits much more severely. Its validation loss is also \n",
        "more noisy.\n",
        "\n",
        "Meanwhile, here are the training losses for our two networks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHwuu6mpx_og",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "3cae6dcf-a12a-4910-8177-885d39a0ae30"
      },
      "source": [
        "#여기는 train loss에 대해서 보고 있으며, 그림에서 보았을 때 거의 0을 찍고 있다는 것을 볼 수 있다.\n",
        "#그래서 bigger가 train set에 대해서 과적합이 심하게 일어났다는 것을 알 수 있다. \n",
        "original_train_loss = original_hist.history['loss']\n",
        "bigger_model_train_loss = bigger_model_hist.history['loss']\n",
        "\n",
        "plt.plot(epochs, original_train_loss, 'b+', label='Original model')\n",
        "plt.plot(epochs, bigger_model_train_loss, 'bo', label='Bigger model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRBq3QNSx_oh"
      },
      "source": [
        "As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the quicker it will be \n",
        "able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large \n",
        "difference between the training and validation loss)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rMMJGO3x_oh"
      },
      "source": [
        "## Adding weight regularization\n",
        "\n",
        "\n",
        "You may be familiar with _Occam's Razor_ principle: given two explanations for something, the explanation most likely to be correct is the \n",
        "\"simplest\" one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some \n",
        "training data and a network architecture, there are multiple sets of weights values (multiple _models_) that could explain the data, and \n",
        "simpler models are less likely to overfit than complex ones.\n",
        "\n",
        "A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer \n",
        "parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity \n",
        "of a network by forcing its weights to only take small values, which makes the distribution of weight values more \"regular\". This is called \n",
        "\"weight regularization\", and it is done by adding to the loss function of the network a _cost_ associated with having large weights. This \n",
        "cost comes in two flavors:\n",
        "\n",
        "* L1 regularization, where the cost added is proportional to the _absolute value of the weights coefficients_ (i.e. to what is called the \n",
        "\"L1 norm\" of the weights).\n",
        "* L2 regularization, where the cost added is proportional to the _square of the value of the weights coefficients_ (i.e. to what is called \n",
        "the \"L2 norm\" of the weights). L2 regularization is also called _weight decay_ in the context of neural networks. Don't let the different \n",
        "name confuse you: weight decay is mathematically the exact same as L2 regularization.\n",
        "\n",
        "In Keras, weight regularization is added by passing _weight regularizer instances_ to layers as keyword arguments. Let's add L2 weight \n",
        "regularization to our movie review classification network:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mzUTmPxvoc2"
      },
      "source": [
        "- You may be familiar with Occam's Razor principle: given two explanations for something, the explanation most likely to be correct is the \"simplest\" one, the one that makes the least amount of assumptions.\n",
        "\n",
        "- simple model이 좋다는 것을 강조를 하고 있다. \n",
        "\n",
        "- A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether, as we saw in the section above). \n",
        "\n",
        "- parameter들이 entropy를 적게 갖는것이 simple한 model이라고 설명하고 있다.그리고 parameter들이 위로 팍 치고 나가는 것을 방지하기 위해서 regularization기법들을 적용해가지고, l1-regularization, l2-regularization을 설명하고 있는데, 여기에서 l1-regularization은 l1-norm(절대값)을 loss뒤에 추가해줘서 parameter들이 l1-norm기준으로 커지는 것을 막아주는 그러한 것이 되고, l2-regularization은  loss에 다가 parameter들에 대한 l2-norm을 regularization으로 줘서 parameter들의 값이 많이 커지지 않겠금 해주는 역할 이라고 할 수 있다.\n",
        "\n",
        "- 그리고 케라스에서는 regularization을 쉽게 달 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaN3TWwcx_oh"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "l2_model = models.Sequential()\n",
        "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), #original data에서는 앞에와 다똑같은데, kernel_regularizer=regularizers.l2(0.001)가 다르다.\n",
        "                          activation='relu', input_shape=(10000,)))  #0.001이 숫자는 l2-regularization을 얼마나 심하게 할 건지를 조절해주는 parameter가 된다.\n",
        "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                          activation='relu'))\n",
        "l2_model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g28Sp8IRx_oh"
      },
      "source": [
        "l2_model.compile(optimizer='rmsprop',   #이 부분에서 컴파일을 해주고\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0CXuzObyELy",
        "outputId": "e6575abb-5a86-46a6-9cb7-66a6a939ec8a"
      },
      "source": [
        "#l2-norm은 재미있는게 parameter의 숫자는 똑같다.\n",
        "l2_model.summary()\n",
        "\n",
        "#코드를 돌려보았을때, Trainable params는 160,305 그래서 parameter의 갯수 자체는 똑같은데, loss를 minimized할때, parameter가 튀지 않는 그 부분을 추가 해줌으로써\n",
        "#더 simple한 모델이 나올 수 있겠금 해준다. 그런 부분이 되겠다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,305\n",
            "Trainable params: 160,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vrfoxNkx_oh"
      },
      "source": [
        "`l2(0.001)` means that every coefficient in the weight matrix of the layer will add `0.001 * weight_coefficient_value` to the total loss of \n",
        "the network. Note that because this penalty is _only added at training time_, the loss for this network will be much higher at training \n",
        "than at test time.\n",
        "\n",
        "Here's the impact of our L2 regularization penalty:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHdcuOivx_oj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7925b3-ac1d-43c5-ba0b-f51d07054995"
      },
      "source": [
        "l2_model_hist = l2_model.fit(x_train, y_train, \n",
        "                             epochs=20,\n",
        "                             batch_size=512,\n",
        "                             validation_data=(x_test, y_test))\n",
        "#이 코들를 돌리게 되면 train acc가 막 올라가지 않는것을 볼 수 있는데, 이게 overfitting이 안된다는 증거이다.\n",
        "#train loss가 막 치고 올라가는 것을 너무 좋은게 아니라 그건 overfitting이 되고 있다라고 판단해도 좋다.\n",
        "#그래서 이렇게 덜 올라가는게 더 좋을 수 도 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 3s 60ms/step - loss: 0.5821 - acc: 0.7431 - val_loss: 0.3723 - val_acc: 0.8850\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.3196 - acc: 0.9102 - val_loss: 0.3331 - val_acc: 0.8897\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.2674 - acc: 0.9243 - val_loss: 0.3585 - val_acc: 0.8731\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.2417 - acc: 0.9338 - val_loss: 0.3444 - val_acc: 0.8806\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.2251 - acc: 0.9410 - val_loss: 0.3425 - val_acc: 0.8837\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.2222 - acc: 0.9394 - val_loss: 0.3512 - val_acc: 0.8808\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.2105 - acc: 0.9461 - val_loss: 0.4245 - val_acc: 0.8562\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.2092 - acc: 0.9461 - val_loss: 0.3708 - val_acc: 0.8751\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.2038 - acc: 0.9474 - val_loss: 0.3727 - val_acc: 0.8760\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.2011 - acc: 0.9478 - val_loss: 0.3788 - val_acc: 0.8746\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.1927 - acc: 0.9531 - val_loss: 0.4075 - val_acc: 0.8686\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.1959 - acc: 0.9495 - val_loss: 0.3955 - val_acc: 0.8718\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1961 - acc: 0.9493 - val_loss: 0.4232 - val_acc: 0.8653\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1877 - acc: 0.9540 - val_loss: 0.3975 - val_acc: 0.8715\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1897 - acc: 0.9536 - val_loss: 0.4019 - val_acc: 0.8702\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1868 - acc: 0.9520 - val_loss: 0.4229 - val_acc: 0.8654\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1807 - acc: 0.9561 - val_loss: 0.4218 - val_acc: 0.8692\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1770 - acc: 0.9572 - val_loss: 0.4434 - val_acc: 0.8629\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1764 - acc: 0.9568 - val_loss: 0.4599 - val_acc: 0.8599\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1724 - acc: 0.9602 - val_loss: 0.4215 - val_acc: 0.8675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvvztDM_x_oj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "4038edf5-29c7-483a-dbab-9f51f7fc6de1"
      },
      "source": [
        "#여기에서는 original_val_loss과 l2_model_val_loss를 비교해서 그림을 그려보았다.\n",
        "l2_model_val_loss = l2_model_hist.history['val_loss']  \n",
        "\n",
        "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
        "plt.plot(epochs, l2_model_val_loss, 'bo', label='L2-regularized model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "#original_model이것은 처음에는 내려가다가 계속해서 overfitting해서 올라가는 반면에, \n",
        "#l2_model_model이것은 overfitting 되는 것을 잘 막아주고 있다는 것을 볼 수 있다.\n",
        "\n",
        "#그런데 그림에서 보이는 것 처럼 original model이 Epochs가 2.5부분에서는 l2_model보다 Validation loss가 낮기 때문에 더 좋다는것을 알 수 있다.\n",
        "#그래서 교수님이 말씀하시길 original mode의 Epochs를 2.5부분 쯤에서 짤라서 사용하는 것도 좋다고 말씀하심.\n",
        "#그래서 nurel network에서는 l1과 l2말고도 dropot을 사용한다고 말씀하심."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5eSLdHnx_ok"
      },
      "source": [
        "\n",
        "\n",
        "As you can see, the model with L2 regularization (dots) has become much more resistant to overfitting than the reference model (crosses), \n",
        "even though both models have the same number of parameters.\n",
        "\n",
        "As alternatives to L2 regularization, you could use one of the following Keras weight regularizers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq3WTNpqx_ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f94946-b41a-490e-dccb-373fcbb4fb9e"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# L1 regularization\n",
        "regularizers.l1(0.001)  #위에서는 l2를 달아 주었지만, 여기에서 처럼 l1으로 해서 코드를 달아 줄 수 있다.\n",
        "\n",
        "# L1 and L2 regularization at the same time\n",
        "regularizers.l1_l2(l1=0.001, l2=0.001)  #그리고 여기에서 처럼 l1과 l2를 같이 사용해서도 사용할 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.regularizers.L1L2 at 0x7f633a2043d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGUIhJGB1eTn"
      },
      "source": [
        "#이거는 L1 regularization을 사용한 방법이다.\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "l1_model = models.Sequential()\n",
        "l1_model.add(layers.Dense(16, kernel_regularizer=regularizers.l1(0.001), \n",
        "                          activation='relu', input_shape=(10000,)))  \n",
        "l1_model.add(layers.Dense(16, kernel_regularizer=regularizers.l1(0.001),\n",
        "                          activation='relu'))\n",
        "l1_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "#그리고 L1과 L2를 같이 사용하기 위해서는 regularizers.l1_l2(l1=0.001, l2=0.001)이렇게 바꾸어 주면 된다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xi0hhVAx_ok"
      },
      "source": [
        "## Adding dropout\n",
        "\n",
        "\n",
        "Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his \n",
        "students at the University of Toronto. Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. setting to zero) a number of \n",
        "output features of the layer during training. Let's say a given layer would normally have returned a vector `[0.2, 0.5, 1.3, 0.8, 1.1]` for a \n",
        "given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. `[0, 0.5, \n",
        "1.3, 0, 1.1]`. The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test \n",
        "time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to \n",
        "balance for the fact that more units are active than at training time.\n",
        "\n",
        "Consider a Numpy matrix containing the output of a layer, `layer_output`, of shape `(batch_size, features)`. At training time, we would be \n",
        "zero-ing out at random a fraction of the values in the matrix:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eymAU8h96MAC"
      },
      "source": [
        "- dropout에 대한 예제를 진행해 본다.\n",
        "\n",
        "-  Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his students at the University of Toronto.\n",
        "\n",
        "- Drpout은 neural networks 쪽에서 가장 효율적이고 가장 많이 사용하는 regularization 방법이고, 성능면에서는 l1, l2보다 dropout이 더 좋다는 것을 알 수 있다. \n",
        "\n",
        "- Dropout의 아이디어는 layer에서 random하게 droping out해가지고 일부 node들을 0으로 만들고 나머지들은 그대로 두는 방법으로 해두게된다.\n",
        "\n",
        "-그래서 왜 이렇게 하면 성능이 좋아지냐? node전체를 사용해서 해 나아가다 보면,일부 node에 쏠리게 되는 현상이 발생하게 된다.그래서 node들이 전부 트레이닝 하게 되면 일부 node들이 쏠리는 현상이 발생하게 되는데, [0, 0.5, 1.3, 0, 1.1]이런식으로 일부 node들을 drop하게 되면 그런 쏠림현상이 많이 줄어들게 되고, 그리고 쏠림현상이 줄어들게 되면 다른쪽으로도 길을 많이 터주기 때문에 그래서 전체적으로보았을 때 일부 node에 쏠리는 현상을 막아 줄 수 있다. 그래서 reguralization되는 효과도 있다.\n",
        "\n",
        "- 그리고 dropout이 randomforest와 비슷하게 앙상블을 사용해서 합쳐줘서 사용하는 것 같은 효과가 있다는 점에서 비슷하다고 볼 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGrz0P0584tM",
        "outputId": "0ba8554c-3db5-480b-8c8a-8df8460ebd54"
      },
      "source": [
        "import numpy as np \n",
        "np.random.randint(0, high=2, size=20)\n",
        "#대충 50%의 점들을 0으로 만들거나 1로 만들어주고 \n",
        "#여기에 layer_output을 곱해주게 되면 1이 있는 부분은 살아남게 되고, 0이 있는 부분은 0으로 되는 것을 알 수 있다. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXPbQSdUx_ok",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "15238cae-e399-4799-b9df-e2feab898e65"
      },
      "source": [
        "# At training time: we drop out 50% of the units in the output\n",
        "layer_output *= np.randint(0, high=2, size=layer_output.shape)   #이거는 이런 느낌으로 구현된다고 생각할 수 있는 예제이다. \n",
        "#이거는 layer_output에 np.randint(0, high=2, size=layer_output.shape)이거를 곱해준다는 것인데 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6071f7dfcafd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# At training time: we drop out 50% of the units in the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlayer_output\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#이거는 이런 느낌으로 구현된다고 생각할 수 있는 예제이다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#이거는 layer_output에 np.randint(0, high=2, size=layer_output.shape)이거를 곱해준다는 것인데\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'layer_output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU84VCIox_ok"
      },
      "source": [
        "\n",
        "At test time, we would be scaling the output down by the dropout rate. Here we scale by 0.5 (because we were previous dropping half the \n",
        "units):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Kd4Jyqo2x_ol"
      },
      "source": [
        "# At test time:\n",
        "layer_output *= 0.5\n",
        "#락다운 안된 전체 애들이 살아 있어서 그래서 전체가 살아 있어서 가중치를 0.5를 곱해서 testing time에서 해주겠다는 것이다. \n",
        "#그리고 여기에서는 위에 0.5를 해주었기 때문에 test time에 0.5를 곱해준다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w7VGyyIx_ol"
      },
      "source": [
        "\n",
        "Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time, which is \n",
        "often the way it is implemented in practice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "CdPaf1zfx_ol"
      },
      "source": [
        "# At training time:\n",
        "layer_output *= np.randint(0, high=2, size=layer_output.shape)\n",
        "# Note that we are scaling *up* rather scaling *down* in this case\n",
        "layer_output /= 0.5  #위에 있는 코드를 똑같이 해주려면 train time에서 layer_output을 0.5로 나누어주면 test time에서 따로 layer_output *= 0.5해주지 않아도 된다.\n",
        "#그래서 실제로 dropout의 구현은 이런느낌으로 되어 있다라는 것을 볼 수 있다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asu9EMfvx_ol"
      },
      "source": [
        "\n",
        "This technique may seem strange and arbitrary. Why would this help reduce overfitting? Geoff Hinton has said that he was inspired, among \n",
        "other things, by a fraud prevention mechanism used by banks -- in his own words: _\"I went to my bank. The tellers kept changing and I asked \n",
        "one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation \n",
        "between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each \n",
        "example would prevent conspiracies and thus reduce overfitting\"_.\n",
        "\n",
        "The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that are not significant (what \n",
        "Hinton refers to as \"conspiracies\"), which the network would start memorizing if no noise was present. \n",
        "\n",
        "In Keras you can introduce dropout in a network via the `Dropout` layer, which gets applied to the output of layer right before it, e.g.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II_16FK7x_ol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "cddafe1e-a708-4d1e-dd64-e7e005ea5572"
      },
      "source": [
        "model.add(layers.Dropout(0.5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-65ed89de200e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pRjKAtTx_ol"
      },
      "source": [
        "Let's add two `Dropout` layers in our IMDB network to see how well they do at reducing overfitting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqVt2o2Hx_om"
      },
      "source": [
        "dpt_model = models.Sequential()\n",
        "dpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "dpt_model.add(layers.Dropout(0.5))   #케라스에서는 간단하게 0.5를 dropout해주면 되겠다라고 하는 것이다. \n",
        "#Dense layers 16에다가 Dropout을 0.5적용한 것이고\n",
        "dpt_model.add(layers.Dense(16, activation='relu'))\n",
        "dpt_model.add(layers.Dropout(0.5))\n",
        "#Dense layers 16에다가 Dropout을 0.5적용한 것이고\n",
        "dpt_model.add(layers.Dense(1, activation='sigmoid'))   #그래서 두 층에 dropout layers를 추가한것을 볼 수 있다.\n",
        "\n",
        "dpt_model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1ZHoSDO_Dvj",
        "outputId": "8543c08b-cffc-4a25-cb16-f108ca0c6cc6"
      },
      "source": [
        "#dropout을 하게 되더라도 모델의 개수에는 변함이 없다.\n",
        "dpt_model.summary()\n",
        "#그래서 Trainable params는 160.305가 되는 것을 볼 수 있다. 그리고 아래 보이는 것 처럼 dropout layer만 추가 된 것을 볼 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,305\n",
            "Trainable params: 160,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP05buc4x_om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f472afca-33bf-4f91-86e6-0480a9add0df"
      },
      "source": [
        "#dropout이 된 것을 train해주게 되면 \n",
        "dpt_model_hist = dpt_model.fit(x_train, y_train,\n",
        "                               epochs=20,\n",
        "                               batch_size=512,\n",
        "                               validation_data=(x_test, y_test))\n",
        "#original model보다는 acc가 더 빠르게 올라기지 않는 것을 볼 수 있다. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 6s 102ms/step - loss: 0.6364 - acc: 0.6197 - val_loss: 0.4408 - val_acc: 0.8667\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 3s 69ms/step - loss: 0.4567 - acc: 0.8101 - val_loss: 0.3370 - val_acc: 0.8836\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.3693 - acc: 0.8658 - val_loss: 0.2991 - val_acc: 0.8875\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.3089 - acc: 0.8969 - val_loss: 0.2844 - val_acc: 0.8886\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.2574 - acc: 0.9159 - val_loss: 0.2902 - val_acc: 0.8793\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.2289 - acc: 0.9282 - val_loss: 0.2962 - val_acc: 0.8880\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.2025 - acc: 0.9392 - val_loss: 0.3140 - val_acc: 0.8819\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1754 - acc: 0.9437 - val_loss: 0.3483 - val_acc: 0.8826\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1654 - acc: 0.9469 - val_loss: 0.3508 - val_acc: 0.8842\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.1512 - acc: 0.9526 - val_loss: 0.3816 - val_acc: 0.8795\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1420 - acc: 0.9550 - val_loss: 0.3990 - val_acc: 0.8803\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1284 - acc: 0.9598 - val_loss: 0.4154 - val_acc: 0.8785\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1321 - acc: 0.9546 - val_loss: 0.4474 - val_acc: 0.8785\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1198 - acc: 0.9617 - val_loss: 0.4813 - val_acc: 0.8766\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1170 - acc: 0.9610 - val_loss: 0.5026 - val_acc: 0.8766\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1166 - acc: 0.9601 - val_loss: 0.5014 - val_acc: 0.8756\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1144 - acc: 0.9620 - val_loss: 0.5262 - val_acc: 0.8739\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1024 - acc: 0.9653 - val_loss: 0.5448 - val_acc: 0.8746\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.0998 - acc: 0.9657 - val_loss: 0.5762 - val_acc: 0.8739\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1032 - acc: 0.9636 - val_loss: 0.5767 - val_acc: 0.8713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV5dPnlYx_om"
      },
      "source": [
        "Let's plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAz5QyQfx_om",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "392fb21c-8a93-4e6e-a7d9-1283d191390a"
      },
      "source": [
        "dpt_model_val_loss = dpt_model_hist.history['val_loss']\n",
        "\n",
        "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
        "plt.plot(epochs, dpt_model_val_loss, 'bo', label='Dropout-regularized model')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "#위에서 acc가 빠르게 올라가지 않는것은 여기에서 original_val_loss와 dpt_model_val_loss를 보았을 때 알 수 있다. \n",
        "#dropout도 물론 overfitting이 일어나지만 original보다 천천히 일어난다는 것을 알 수 있다. \n",
        "#그리고 그림에서 보았을때, l1과 l2보다 Epochs 2.5에서 5.0사이가 dropout했을 때 좀더 좋아 진것을 볼 수 있다. 즉 Validation loss가 더 좋아졌다.\n",
        "#이래서 l1과 l2보다 dropout을 많이 사용을 한다. 그 이유가 성능면에서도 더 좋아지는 경우가 많아서 overfitting을 방지해주면서도 높은 성능을 유지해주는 그런 기법이여서\n",
        "#neural network에서는 많이 사용하고 있다. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9bwTmFqx_om"
      },
      "source": [
        "\n",
        "Again, a clear improvement over the reference network.\n",
        "\n",
        "To recap: here the most common ways to prevent overfitting in neural networks:\n",
        "\n",
        "* Getting more training data.\n",
        "* Reducing the capacity of the network.\n",
        "* Adding weight regularization.\n",
        "* Adding dropout."
      ]
    }
  ]
}