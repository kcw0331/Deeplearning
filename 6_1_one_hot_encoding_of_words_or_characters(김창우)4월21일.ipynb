{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "6.1-one-hot-encoding-of-words-or-characters(김창우)4월21일.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcw0331/Deeplearning/blob/main/6_1_one_hot_encoding_of_words_or_characters(%EA%B9%80%EC%B0%BD%EC%9A%B0)4%EC%9B%9421%EC%9D%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv_MjZ_kVgzv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de4c0ac5-b81a-40cc-988d-88dacfafd5ec"
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWnKd7CKVgz0"
      },
      "source": [
        "# One-hot encoding of words or characters\n",
        "\n",
        "This notebook contains the first code sample found in Chapter 6, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "One-hot encoding is the most common, most basic way to turn a token into a vector. You already saw it in action in our initial IMDB and \n",
        "Reuters examples from chapter 3 (done with words, in our case). It consists in associating a unique integer index to every word, then \n",
        "turning this integer index i into a binary vector of size N, the size of the vocabulary, that would be all-zeros except for the i-th \n",
        "entry, which would be 1.\n",
        "\n",
        "Of course, one-hot encoding can be done at the character level as well. To unambiguously drive home what one-hot encoding is and how to \n",
        "implement it, here are two toy examples of one-hot encoding: one for words, the other for characters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vFZXGL0Vgz1"
      },
      "source": [
        "Word level one-hot encoding (toy example):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkZqGtrj1Hte"
      },
      "source": [
        "- one hot encoding에 대한 실습을 시작한다.\n",
        "- one hot encoding은 keras에도 있지만 실제 손으로 직접하게 되면 어떻게 될지에 대해 보게 되는 코드라고 말씀하심.\n",
        "- 그리고 교수님이 코드에 대한 이해를 돕기 위해서 코드를 잘라서 본다고 하심."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PiYWvZx1lTi"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# This is our initial data; one entry per \"sample\"\n",
        "# (in this toy example, a \"sample\" is just a sentence, but\n",
        "# it could be an entire document).\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "#samples = ['The cat sat on the mat.'.lower(), 'The dog ate my homework.'.lower()]\n",
        "# .lower()를 하게되면, The에서 T부분이 소문자 t로바뀌게 되는 것을 볼 수 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFsWjMZA1r0Q",
        "outputId": "d57fb1d5-0216-4d1d-fab3-d232c3d550fe"
      },
      "source": [
        "samples   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The cat sat on the mat.', 'The dog ate my homework.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jrAYHKU1ucc"
      },
      "source": [
        "- samples에는 The cat sat on the mat이랑 The dog ate my homework가 있는다는 것을 알 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngtIZ_9Y19-f"
      },
      "source": [
        "token_index = {}\n",
        "for sample in samples:\n",
        "    # We simply tokenize the samples via the `split` method.\n",
        "    # in real life, we would also strip punctuation and special characters\n",
        "    # from the samples.\n",
        "    for word in sample.split():\n",
        "        if word not in token_index:\n",
        "            # Assign a unique index to each unique word\n",
        "            token_index[word] = len(token_index) + 1\n",
        "            # Note that we don't attribute index 0 to anything."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haJFItmf2P56",
        "outputId": "d27ee62b-b7f6-47a7-f325-116834d30ebb"
      },
      "source": [
        "token_index   #이거를 출력하게 되면 The와 the가 구분이 안되는 것을 볼 수 있다. 그래서 위에서 .lower()를 하게 되면 \n",
        "#The가 the로 바뀌게 되면서 대문자 소문자 구분을 할 수 있게 된다. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'The': 1,\n",
              " 'ate': 8,\n",
              " 'cat': 2,\n",
              " 'dog': 7,\n",
              " 'homework.': 10,\n",
              " 'mat.': 6,\n",
              " 'my': 9,\n",
              " 'on': 4,\n",
              " 'sat': 3,\n",
              " 'the': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypEZiFkl1-qC"
      },
      "source": [
        "- 그리고 여기에서는 token_index라는 dictionary를 만든다음에 그리고 token_index를 보게 되면 값이 하나 하나 들어가 있다는 것을 알 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3wxfAg43fyE"
      },
      "source": [
        "max_length = 10\n",
        "\n",
        "# This is where we store our results:\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i, j, index] = 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8-jkIAE3t7M",
        "outputId": "0bdd9237-05de-401e-927a-5b71107bdd60"
      },
      "source": [
        "results \n",
        "#아래 보이는 첫번째 matrix는 The cat sat on the mat에 대한 matrix이다. \n",
        "#그리고 두번째 matrix는 the dog ate my homework에 대한 matix이다. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFXzV7Oc3kma"
      },
      "source": [
        "- 위에서 token을 추출하고 난 다음에는 results에다가 결과를 집어 넣어 준다. \n",
        "- 그래서 one-hot-encoding이 results결과에 보이는 matrix 처럼 결과가 출력된다는 것을 볼 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y0o-2MA4nhG"
      },
      "source": [
        "- 그래서 아래 보이는 코드는 Word level one-hot encoding을 사용할 때 사용하는 방법이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIjabHAlVgz1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# This is our initial data; one entry per \"sample\"\n",
        "# (in this toy example, a \"sample\" is just a sentence, but\n",
        "# it could be an entire document).\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# First, build an index of all tokens in the data.\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "    # We simply tokenize the samples via the `split` method.\n",
        "    # in real life, we would also strip punctuation and special characters\n",
        "    # from the samples.\n",
        "    for word in sample.split():\n",
        "        if word not in token_index:\n",
        "            # Assign a unique index to each unique word\n",
        "            token_index[word] = len(token_index) + 1\n",
        "            # Note that we don't attribute index 0 to anything.\n",
        "\n",
        "# Next, we vectorize our samples.\n",
        "# We will only consider the first `max_length` words in each sample.\n",
        "max_length = 10\n",
        "\n",
        "# This is where we store our results:\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i, j, index] = 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqbKKadDVgz2"
      },
      "source": [
        "Character level one-hot encoding (toy example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiTYhn085TYE"
      },
      "source": [
        "- 이런식으로 저장하면서 하는게 Character level one-hot encoding방식이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ar9-d3b5azK"
      },
      "source": [
        "import string\n",
        "characters = string.printable  # All printable ASCII characters."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbyaxynC5g-8",
        "outputId": "8ef78beb-f50b-4683-b083-8a97b51d756f"
      },
      "source": [
        "len(characters) #characters가 100개가 있다는 것을 알 수 있다. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tQE16mjVgz2"
      },
      "source": [
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable  # All printable ASCII characters.\n",
        "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
        "\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, character in enumerate(sample[:max_length]):\n",
        "        index = token_index.get(character)\n",
        "        results[i, j, index] = 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2tyuN1wiHCS",
        "outputId": "32178d28-9d2a-47db-e2ab-5e69c2cc5356"
      },
      "source": [
        "token_index.get(character)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8ie5WHKgFVs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fG92oNF5zQa",
        "outputId": "3c1812be-46c3-496a-cbb6-67cdfab2b76b"
      },
      "source": [
        "results   #그래서 results를 하게 되면 100차원의 결과가 나오면 The cat sta on the mat과 The dog ate my homework가 아래 보이는\n",
        "# matrix부분에 들어가는 것을 볼 수 있다. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5o5k1ScVgz2"
      },
      "source": [
        "Note that Keras has built-in utilities for doing one-hot encoding text at the word level or character level, starting from raw text data. \n",
        "This is what you should actually be using, as it will take care of a number of important features, such as stripping special characters \n",
        "from strings, or only taking into the top N most common words in your dataset (a common restriction to avoid dealing with very large input \n",
        "vector spaces)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnTBg0e6Vgz2"
      },
      "source": [
        "Using Keras for word-level one-hot encoding:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt3WMcMj6JZe"
      },
      "source": [
        "- 이 부분은 Using Kearas for word-level one-hot encoding 부분이다.\n",
        "- 그리고 이 부분에 대한 코드를 설명하기 위해서 코드를 따로 떼어내서 설명을 하신다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coj8YUrk6XqL"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# We create a tokenizer, configured to only take\n",
        "# into account the top-1000 most common words\n",
        "tokenizer = Tokenizer(num_words=1000)  #Tokenizer가 num_words가 1000개정도 할 수 있도록 쓸것이다. \n",
        "#가장 빈도가 높은 1,000개의 단어만 선택하도록 Tokenizer 객체를 만든다. 2021년 4월 27일 복습할때 적음."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfK9zmRQ6ZH3"
      },
      "source": [
        "- 위의 코드에서는 tokenizer를 정의를 한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuBc1xIA6wjP"
      },
      "source": [
        "tokenizer.fit_on_texts(samples)   #이거는 sampeles를 이용해서 tokenizer를 해준다. \n",
        "#fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다. 2021년4월27일 복습할때 적음."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-fPgXQ47ABP"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(samples)   #texts들이 sequences가 어떻게 되는지 보면 \n",
        "#.texts_to_sequences <- 이것을 사용해서 텍스트를 시퀀스로 만드는 작업을 한다. 2021년 4월 27일 복습할때 적음."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGIoAsss7HFH",
        "outputId": "7d16cba5-cf57-4dc5-90d4-9b4f63fbafac"
      },
      "source": [
        "sequences   #이렇게 sequences가 되는 것을 볼 수 있다. \n",
        "#아래 보이는게 첫번째 거는 The cat sat on the mat이 되고, 두번째 거는 The dog ate my homework가 되는 것을 볼 수 있다. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJVVhqAZ7OJb"
      },
      "source": [
        "- 앞에서는 lower()가 자동으로 되지 않았던 것을 볼 수 있었는데, 여기에서는 lower()가 자동으로 되는 것을 볼 수 있다. \n",
        "- keras에서는 The와 the를 똑같은 걸로 인식을 하게 된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ckQow3j7_UT"
      },
      "source": [
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "#.texts_to_matrix <- 이것을 사용하여 one-hot-encoding을 사용할 수 있다. 2021년 4월 27일 복습할때 적음."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JpH8fXC8AFe",
        "outputId": "dfa7290d-5946-4758-ad15-7fa4fbcb41f8"
      },
      "source": [
        "one_hot_results[0]  \n",
        "#이걸해주게 되면 The cat sat on the mat가 있는 자리에 1을 하게 되고 없는자리에는 0이 된다.\n",
        "#그리고 The dog ate my homework가 있는 자리에 1을 하게 되고 없는 자리에는 0이 된다.\n",
        "#len(one_hot_results[0])를 하게 되면 1000차원이 되는 것을 볼 수 있다. 왜냐하면 위에 있는 이 코드 tokenizer = Tokenizer(num_words=1000)에서 1000을 사용했기 때문이다. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIgwGGi89PS_",
        "outputId": "acf7c122-4d65-4463-c5dc-469ae7eb677f"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "#.word_index <- 속성은 단어와 숫자의 키-값 쌍을 포함하는 딕셔너리를 반환합니다. 2021년 4월 27일 복습할때 적음."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM6jfkEI9P45",
        "outputId": "5a033962-c9f5-426d-f2d3-8e9a77c83467"
      },
      "source": [
        "word_index  #word_index를 하게 되면 이렇게 되는 것을 볼 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ate': 7,\n",
              " 'cat': 2,\n",
              " 'dog': 6,\n",
              " 'homework': 9,\n",
              " 'mat': 5,\n",
              " 'my': 8,\n",
              " 'on': 4,\n",
              " 'sat': 3,\n",
              " 'the': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnnulMqzVgz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c362dee3-6add-43f6-b08e-13381d8587f4"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# We create a tokenizer, configured to only take\n",
        "# into account the top-1000 most common words\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "# This builds the word index\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "# This turns strings into lists of integer indices.\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "\n",
        "# You could also directly get the one-hot binary representations.\n",
        "# Note that other vectorization modes than one-hot encoding are supported!\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "\n",
        "# This is how you can recover the word index that was computed\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-yK1A7s83Dc"
      },
      "source": [
        "- one_hot_results이거를 할 때는 tokenizer = Tokenizer(num_words=1000), tokenizer.fit_on_texts(samples), sequences, tokenizer.texts_to_matrix(samples, mode='binary')이게 들어가야 한다고 말씀하심.\n",
        "- 사실 tokenizer에는 사용할 수 있는게 많이 있는데 자신에게 맞는거를 찾기 위해서는 구글에서 tokenizer에 대해 많이 찾아 본다음에 자신에게 맞는것을 찾아서 사용하면 된다고 말씀하심.\n",
        "\n",
        "# 교수님이 keras에 있는 function을 사용하는게 더 좋다고 말씀하심."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SYQTT-EVgz3"
      },
      "source": [
        "\n",
        "A variant of one-hot encoding is the so-called \"one-hot hashing trick\", which can be used when the number of unique tokens in your \n",
        "vocabulary is too large to handle explicitly. Instead of explicitly assigning an index to each word and keeping a reference of these \n",
        "indices in a dictionary, one may hash words into vectors of fixed size. This is typically done with a very lightweight hashing function. \n",
        "The main advantage of this method is that it does away with maintaining an explicit word index, which \n",
        "saves memory and allows online encoding of the data (starting to generate token vectors right away, before having seen all of the available \n",
        "data). The one drawback of this method is that it is susceptible to \"hash collisions\": two different words may end up with the same hash, \n",
        "and subsequently any machine learning model looking at these hashes won't be able to tell the difference between these words. The likelihood \n",
        "of hash collisions decreases when the dimensionality of the hashing space is much larger than the total number of unique tokens being hashed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmPyf0XKVgz4"
      },
      "source": [
        "Word-level one-hot encoding with hashing trick (toy example):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAxDp3G5-JzU"
      },
      "source": [
        "- 이거는 hashing trick을 사용하는 방법이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ECLACgYVgz4"
      },
      "source": [
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# We will store our words as vectors of size 1000.\n",
        "# Note that if you have close to 1000 words (or more)\n",
        "# you will start seeing many hash collisions, which\n",
        "# will decrease the accuracy of this encoding method.\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        # Hash the word into a \"random\" integer index\n",
        "        # that is between 0 and 1000\n",
        "        index = abs(hash(word)) % dimensionality   #여기에서는 hash(word) hash function를 사용해서 1000 dimensionality를 사용해야 한다. index가 0에서 1000사이의 값을 갖도록 해준다. \n",
        "        results[i, j, index] = 1.  #그래서 결과가 1000차원이 되도록 해준다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NByCOec-2ub"
      },
      "source": [
        "- 이거한 다음에 강의 노트 페이지 11로 넘어가서 Using word embeddings에 대해 수업을 진행하심. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0x4wwAnVgz4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}