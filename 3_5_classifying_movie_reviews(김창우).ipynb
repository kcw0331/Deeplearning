{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "3_5_classifying_movie_reviews(김창우).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcw0331/Deeplearning/blob/main/3_5_classifying_movie_reviews(%EA%B9%80%EC%B0%BD%EC%9A%B0).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7w--vaHB8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3c917414-5ef4-4b87-cd01-8eb3acac92c8"
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.keras.__version__   #tensorflow안에 있는 keras를 사용하고 싶어서 tensorflow.keras.라고 코드를 적었다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIZ5qtUZB8fh"
      },
      "source": [
        "# Classifying movie reviews: a binary classification example\n",
        "\n",
        "This notebook contains the code samples found in Chapter 3, Section 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        "Two-class classification, or binary classification, may be the most widely applied kind of machine learning problem. In this example, we \n",
        "will learn to classify movie reviews into \"positive\" reviews and \"negative\" reviews, just based on the text content of the reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVc555pDB8fh"
      },
      "source": [
        "## The IMDB dataset\n",
        "\n",
        "\n",
        "We'll be working with \"IMDB dataset\", a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 \n",
        "reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n",
        "\n",
        "Why do we have these two separate training and test sets? You should never test a machine learning model on the same data that you used to \n",
        "train it! Just because a model performs well on its training data doesn't mean that it will perform well on data it has never seen, and \n",
        "what you actually care about is your model's performance on new data (since you already know the labels of your training data -- obviously \n",
        "you don't need your model to predict those). For instance, it is possible that your model could end up merely _memorizing_ a mapping between \n",
        "your training samples and their targets -- which would be completely useless for the task of predicting targets for data never seen before. \n",
        "We will go over this point in much more detail in the next chapter.\n",
        "\n",
        "Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) \n",
        "have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n",
        "\n",
        "The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded to your machine):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3FRh_E8B8fi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585cb087-b80c-45e6-a307-9dc42beaca59"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)   #review data를 읽는 부분에 대해서 나오게 된다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcItEfKrB8fi"
      },
      "source": [
        "\n",
        "The argument `num_words=10000` means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words \n",
        "will be discarded. This allows us to work with vector data of manageable size.\n",
        "\n",
        "The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). \n",
        "`train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for \"negative\" and 1 stands for \"positive\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEAY9Xn1B8fi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edb183d-49ca-4eb2-bf1b-290e4826de6c"
      },
      "source": [
        "train_data[0]  #첫번째 train data가 뭔지에 대한 것이다.  여기에서 나오는 숫자들이 단어를 의미하는 것이다. 여기에서 숫자의 종류는 10000개가 될 것이다.\n",
        "#왜냐하면 위에서 num_words=10000개를 했기 때문이다. #표현 가능한 단어가 10000개 있다는 것이다. 자주쓰는 데이터가 10000개가 포함되고 10000개가 넘는 것들도 있다.\n",
        "#하지만 10000개가 넘는것은 data set에 포함이 되지 않는다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmLpVv_nB8fj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b301555f-a7b4-4502-9904-38bb88a41905"
      },
      "source": [
        "train_labels[0]   #train_label은 0아니면 1로 표현이 된다. positive아니면 negative가 된다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh_eINM2B8fj"
      },
      "source": [
        "Since we restricted ourselves to the top 10,000 most frequent words, no word index will exceed 10,000:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNQTOTSNB8fj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34197e89-67fb-46e5-88bc-1dfb25d0bad5"
      },
      "source": [
        "max([max(sequence) for sequence in train_data])   #사용가능한 숫자가 10000개가 있다는 것이다.그래서 0부터 9999까지 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_6HSrW6B8fj"
      },
      "source": [
        "For kicks, here's how you can quickly decode one of these reviews back to English words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeykBVgbdLea"
      },
      "source": [
        "###word_index = imdb.get_word_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erV6WiKcdLze",
        "outputId": "8c2bc138-38fa-4eca-ef52-8df4a4a3edbe"
      },
      "source": [
        "###word_index   #이 word_index는 word가 어떤 숫자에 매핑하는지에 관련된 정보이다.  그리고 10000이라는 리스트립션을 풀면 모든 정보들이 나오게 된다. \n",
        "#10000번째 까지가 자주 사용되는 것들이고, 10000번 이상인 것들은 자주 사용되지 않는 것들이다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fawn': 34701,\n",
              " 'tsukino': 52006,\n",
              " 'nunnery': 52007,\n",
              " 'sonja': 16816,\n",
              " 'vani': 63951,\n",
              " 'woods': 1408,\n",
              " 'spiders': 16115,\n",
              " 'hanging': 2345,\n",
              " 'woody': 2289,\n",
              " 'trawling': 52008,\n",
              " \"hold's\": 52009,\n",
              " 'comically': 11307,\n",
              " 'localized': 40830,\n",
              " 'disobeying': 30568,\n",
              " \"'royale\": 52010,\n",
              " \"harpo's\": 40831,\n",
              " 'canet': 52011,\n",
              " 'aileen': 19313,\n",
              " 'acurately': 52012,\n",
              " \"diplomat's\": 52013,\n",
              " 'rickman': 25242,\n",
              " 'arranged': 6746,\n",
              " 'rumbustious': 52014,\n",
              " 'familiarness': 52015,\n",
              " \"spider'\": 52016,\n",
              " 'hahahah': 68804,\n",
              " \"wood'\": 52017,\n",
              " 'transvestism': 40833,\n",
              " \"hangin'\": 34702,\n",
              " 'bringing': 2338,\n",
              " 'seamier': 40834,\n",
              " 'wooded': 34703,\n",
              " 'bravora': 52018,\n",
              " 'grueling': 16817,\n",
              " 'wooden': 1636,\n",
              " 'wednesday': 16818,\n",
              " \"'prix\": 52019,\n",
              " 'altagracia': 34704,\n",
              " 'circuitry': 52020,\n",
              " 'crotch': 11585,\n",
              " 'busybody': 57766,\n",
              " \"tart'n'tangy\": 52021,\n",
              " 'burgade': 14129,\n",
              " 'thrace': 52023,\n",
              " \"tom's\": 11038,\n",
              " 'snuggles': 52025,\n",
              " 'francesco': 29114,\n",
              " 'complainers': 52027,\n",
              " 'templarios': 52125,\n",
              " '272': 40835,\n",
              " '273': 52028,\n",
              " 'zaniacs': 52130,\n",
              " '275': 34706,\n",
              " 'consenting': 27631,\n",
              " 'snuggled': 40836,\n",
              " 'inanimate': 15492,\n",
              " 'uality': 52030,\n",
              " 'bronte': 11926,\n",
              " 'errors': 4010,\n",
              " 'dialogs': 3230,\n",
              " \"yomada's\": 52031,\n",
              " \"madman's\": 34707,\n",
              " 'dialoge': 30585,\n",
              " 'usenet': 52033,\n",
              " 'videodrome': 40837,\n",
              " \"kid'\": 26338,\n",
              " 'pawed': 52034,\n",
              " \"'girlfriend'\": 30569,\n",
              " \"'pleasure\": 52035,\n",
              " \"'reloaded'\": 52036,\n",
              " \"kazakos'\": 40839,\n",
              " 'rocque': 52037,\n",
              " 'mailings': 52038,\n",
              " 'brainwashed': 11927,\n",
              " 'mcanally': 16819,\n",
              " \"tom''\": 52039,\n",
              " 'kurupt': 25243,\n",
              " 'affiliated': 21905,\n",
              " 'babaganoosh': 52040,\n",
              " \"noe's\": 40840,\n",
              " 'quart': 40841,\n",
              " 'kids': 359,\n",
              " 'uplifting': 5034,\n",
              " 'controversy': 7093,\n",
              " 'kida': 21906,\n",
              " 'kidd': 23379,\n",
              " \"error'\": 52041,\n",
              " 'neurologist': 52042,\n",
              " 'spotty': 18510,\n",
              " 'cobblers': 30570,\n",
              " 'projection': 9878,\n",
              " 'fastforwarding': 40842,\n",
              " 'sters': 52043,\n",
              " \"eggar's\": 52044,\n",
              " 'etherything': 52045,\n",
              " 'gateshead': 40843,\n",
              " 'airball': 34708,\n",
              " 'unsinkable': 25244,\n",
              " 'stern': 7180,\n",
              " \"cervi's\": 52046,\n",
              " 'dnd': 40844,\n",
              " 'dna': 11586,\n",
              " 'insecurity': 20598,\n",
              " \"'reboot'\": 52047,\n",
              " 'trelkovsky': 11037,\n",
              " 'jaekel': 52048,\n",
              " 'sidebars': 52049,\n",
              " \"sforza's\": 52050,\n",
              " 'distortions': 17633,\n",
              " 'mutinies': 52051,\n",
              " 'sermons': 30602,\n",
              " '7ft': 40846,\n",
              " 'boobage': 52052,\n",
              " \"o'bannon's\": 52053,\n",
              " 'populations': 23380,\n",
              " 'chulak': 52054,\n",
              " 'mesmerize': 27633,\n",
              " 'quinnell': 52055,\n",
              " 'yahoo': 10307,\n",
              " 'meteorologist': 52057,\n",
              " 'beswick': 42577,\n",
              " 'boorman': 15493,\n",
              " 'voicework': 40847,\n",
              " \"ster'\": 52058,\n",
              " 'blustering': 22922,\n",
              " 'hj': 52059,\n",
              " 'intake': 27634,\n",
              " 'morally': 5621,\n",
              " 'jumbling': 40849,\n",
              " 'bowersock': 52060,\n",
              " \"'porky's'\": 52061,\n",
              " 'gershon': 16821,\n",
              " 'ludicrosity': 40850,\n",
              " 'coprophilia': 52062,\n",
              " 'expressively': 40851,\n",
              " \"india's\": 19500,\n",
              " \"post's\": 34710,\n",
              " 'wana': 52063,\n",
              " 'wang': 5283,\n",
              " 'wand': 30571,\n",
              " 'wane': 25245,\n",
              " 'edgeways': 52321,\n",
              " 'titanium': 34711,\n",
              " 'pinta': 40852,\n",
              " 'want': 178,\n",
              " 'pinto': 30572,\n",
              " 'whoopdedoodles': 52065,\n",
              " 'tchaikovsky': 21908,\n",
              " 'travel': 2103,\n",
              " \"'victory'\": 52066,\n",
              " 'copious': 11928,\n",
              " 'gouge': 22433,\n",
              " \"chapters'\": 52067,\n",
              " 'barbra': 6702,\n",
              " 'uselessness': 30573,\n",
              " \"wan'\": 52068,\n",
              " 'assimilated': 27635,\n",
              " 'petiot': 16116,\n",
              " 'most\\x85and': 52069,\n",
              " 'dinosaurs': 3930,\n",
              " 'wrong': 352,\n",
              " 'seda': 52070,\n",
              " 'stollen': 52071,\n",
              " 'sentencing': 34712,\n",
              " 'ouroboros': 40853,\n",
              " 'assimilates': 40854,\n",
              " 'colorfully': 40855,\n",
              " 'glenne': 27636,\n",
              " 'dongen': 52072,\n",
              " 'subplots': 4760,\n",
              " 'kiloton': 52073,\n",
              " 'chandon': 23381,\n",
              " \"effect'\": 34713,\n",
              " 'snugly': 27637,\n",
              " 'kuei': 40856,\n",
              " 'welcomed': 9092,\n",
              " 'dishonor': 30071,\n",
              " 'concurrence': 52075,\n",
              " 'stoicism': 23382,\n",
              " \"guys'\": 14896,\n",
              " \"beroemd'\": 52077,\n",
              " 'butcher': 6703,\n",
              " \"melfi's\": 40857,\n",
              " 'aargh': 30623,\n",
              " 'playhouse': 20599,\n",
              " 'wickedly': 11308,\n",
              " 'fit': 1180,\n",
              " 'labratory': 52078,\n",
              " 'lifeline': 40859,\n",
              " 'screaming': 1927,\n",
              " 'fix': 4287,\n",
              " 'cineliterate': 52079,\n",
              " 'fic': 52080,\n",
              " 'fia': 52081,\n",
              " 'fig': 34714,\n",
              " 'fmvs': 52082,\n",
              " 'fie': 52083,\n",
              " 'reentered': 52084,\n",
              " 'fin': 30574,\n",
              " 'doctresses': 52085,\n",
              " 'fil': 52086,\n",
              " 'zucker': 12606,\n",
              " 'ached': 31931,\n",
              " 'counsil': 52088,\n",
              " 'paterfamilias': 52089,\n",
              " 'songwriter': 13885,\n",
              " 'shivam': 34715,\n",
              " 'hurting': 9654,\n",
              " 'effects': 299,\n",
              " 'slauther': 52090,\n",
              " \"'flame'\": 52091,\n",
              " 'sommerset': 52092,\n",
              " 'interwhined': 52093,\n",
              " 'whacking': 27638,\n",
              " 'bartok': 52094,\n",
              " 'barton': 8775,\n",
              " 'frewer': 21909,\n",
              " \"fi'\": 52095,\n",
              " 'ingrid': 6192,\n",
              " 'stribor': 30575,\n",
              " 'approporiately': 52096,\n",
              " 'wobblyhand': 52097,\n",
              " 'tantalisingly': 52098,\n",
              " 'ankylosaurus': 52099,\n",
              " 'parasites': 17634,\n",
              " 'childen': 52100,\n",
              " \"jenkins'\": 52101,\n",
              " 'metafiction': 52102,\n",
              " 'golem': 17635,\n",
              " 'indiscretion': 40860,\n",
              " \"reeves'\": 23383,\n",
              " \"inamorata's\": 57781,\n",
              " 'brittannica': 52104,\n",
              " 'adapt': 7916,\n",
              " \"russo's\": 30576,\n",
              " 'guitarists': 48246,\n",
              " 'abbott': 10553,\n",
              " 'abbots': 40861,\n",
              " 'lanisha': 17649,\n",
              " 'magickal': 40863,\n",
              " 'mattter': 52105,\n",
              " \"'willy\": 52106,\n",
              " 'pumpkins': 34716,\n",
              " 'stuntpeople': 52107,\n",
              " 'estimate': 30577,\n",
              " 'ugghhh': 40864,\n",
              " 'gameplay': 11309,\n",
              " \"wern't\": 52108,\n",
              " \"n'sync\": 40865,\n",
              " 'sickeningly': 16117,\n",
              " 'chiara': 40866,\n",
              " 'disturbed': 4011,\n",
              " 'portmanteau': 40867,\n",
              " 'ineffectively': 52109,\n",
              " \"duchonvey's\": 82143,\n",
              " \"nasty'\": 37519,\n",
              " 'purpose': 1285,\n",
              " 'lazers': 52112,\n",
              " 'lightened': 28105,\n",
              " 'kaliganj': 52113,\n",
              " 'popularism': 52114,\n",
              " \"damme's\": 18511,\n",
              " 'stylistics': 30578,\n",
              " 'mindgaming': 52115,\n",
              " 'spoilerish': 46449,\n",
              " \"'corny'\": 52117,\n",
              " 'boerner': 34718,\n",
              " 'olds': 6792,\n",
              " 'bakelite': 52118,\n",
              " 'renovated': 27639,\n",
              " 'forrester': 27640,\n",
              " \"lumiere's\": 52119,\n",
              " 'gaskets': 52024,\n",
              " 'needed': 884,\n",
              " 'smight': 34719,\n",
              " 'master': 1297,\n",
              " \"edie's\": 25905,\n",
              " 'seeber': 40868,\n",
              " 'hiya': 52120,\n",
              " 'fuzziness': 52121,\n",
              " 'genesis': 14897,\n",
              " 'rewards': 12607,\n",
              " 'enthrall': 30579,\n",
              " \"'about\": 40869,\n",
              " \"recollection's\": 52122,\n",
              " 'mutilated': 11039,\n",
              " 'fatherlands': 52123,\n",
              " \"fischer's\": 52124,\n",
              " 'positively': 5399,\n",
              " '270': 34705,\n",
              " 'ahmed': 34720,\n",
              " 'zatoichi': 9836,\n",
              " 'bannister': 13886,\n",
              " 'anniversaries': 52127,\n",
              " \"helm's\": 30580,\n",
              " \"'work'\": 52128,\n",
              " 'exclaimed': 34721,\n",
              " \"'unfunny'\": 52129,\n",
              " '274': 52029,\n",
              " 'feeling': 544,\n",
              " \"wanda's\": 52131,\n",
              " 'dolan': 33266,\n",
              " '278': 52133,\n",
              " 'peacoat': 52134,\n",
              " 'brawny': 40870,\n",
              " 'mishra': 40871,\n",
              " 'worlders': 40872,\n",
              " 'protags': 52135,\n",
              " 'skullcap': 52136,\n",
              " 'dastagir': 57596,\n",
              " 'affairs': 5622,\n",
              " 'wholesome': 7799,\n",
              " 'hymen': 52137,\n",
              " 'paramedics': 25246,\n",
              " 'unpersons': 52138,\n",
              " 'heavyarms': 52139,\n",
              " 'affaire': 52140,\n",
              " 'coulisses': 52141,\n",
              " 'hymer': 40873,\n",
              " 'kremlin': 52142,\n",
              " 'shipments': 30581,\n",
              " 'pixilated': 52143,\n",
              " \"'00s\": 30582,\n",
              " 'diminishing': 18512,\n",
              " 'cinematic': 1357,\n",
              " 'resonates': 14898,\n",
              " 'simplify': 40874,\n",
              " \"nature'\": 40875,\n",
              " 'temptresses': 40876,\n",
              " 'reverence': 16822,\n",
              " 'resonated': 19502,\n",
              " 'dailey': 34722,\n",
              " '2\\x85': 52144,\n",
              " 'treize': 27641,\n",
              " 'majo': 52145,\n",
              " 'kiya': 21910,\n",
              " 'woolnough': 52146,\n",
              " 'thanatos': 39797,\n",
              " 'sandoval': 35731,\n",
              " 'dorama': 40879,\n",
              " \"o'shaughnessy\": 52147,\n",
              " 'tech': 4988,\n",
              " 'fugitives': 32018,\n",
              " 'teck': 30583,\n",
              " \"'e'\": 76125,\n",
              " 'doesn’t': 40881,\n",
              " 'purged': 52149,\n",
              " 'saying': 657,\n",
              " \"martians'\": 41095,\n",
              " 'norliss': 23418,\n",
              " 'dickey': 27642,\n",
              " 'dicker': 52152,\n",
              " \"'sependipity\": 52153,\n",
              " 'padded': 8422,\n",
              " 'ordell': 57792,\n",
              " \"sturges'\": 40882,\n",
              " 'independentcritics': 52154,\n",
              " 'tempted': 5745,\n",
              " \"atkinson's\": 34724,\n",
              " 'hounded': 25247,\n",
              " 'apace': 52155,\n",
              " 'clicked': 15494,\n",
              " \"'humor'\": 30584,\n",
              " \"martino's\": 17177,\n",
              " \"'supporting\": 52156,\n",
              " 'warmongering': 52032,\n",
              " \"zemeckis's\": 34725,\n",
              " 'lube': 21911,\n",
              " 'shocky': 52157,\n",
              " 'plate': 7476,\n",
              " 'plata': 40883,\n",
              " 'sturgess': 40884,\n",
              " \"nerds'\": 40885,\n",
              " 'plato': 20600,\n",
              " 'plath': 34726,\n",
              " 'platt': 40886,\n",
              " 'mcnab': 52159,\n",
              " 'clumsiness': 27643,\n",
              " 'altogether': 3899,\n",
              " 'massacring': 42584,\n",
              " 'bicenntinial': 52160,\n",
              " 'skaal': 40887,\n",
              " 'droning': 14360,\n",
              " 'lds': 8776,\n",
              " 'jaguar': 21912,\n",
              " \"cale's\": 34727,\n",
              " 'nicely': 1777,\n",
              " 'mummy': 4588,\n",
              " \"lot's\": 18513,\n",
              " 'patch': 10086,\n",
              " 'kerkhof': 50202,\n",
              " \"leader's\": 52161,\n",
              " \"'movie\": 27644,\n",
              " 'uncomfirmed': 52162,\n",
              " 'heirloom': 40888,\n",
              " 'wrangle': 47360,\n",
              " 'emotion\\x85': 52163,\n",
              " \"'stargate'\": 52164,\n",
              " 'pinoy': 40889,\n",
              " 'conchatta': 40890,\n",
              " 'broeke': 41128,\n",
              " 'advisedly': 40891,\n",
              " \"barker's\": 17636,\n",
              " 'descours': 52166,\n",
              " 'lots': 772,\n",
              " 'lotr': 9259,\n",
              " 'irs': 9879,\n",
              " 'lott': 52167,\n",
              " 'xvi': 40892,\n",
              " 'irk': 34728,\n",
              " 'irl': 52168,\n",
              " 'ira': 6887,\n",
              " 'belzer': 21913,\n",
              " 'irc': 52169,\n",
              " 'ire': 27645,\n",
              " 'requisites': 40893,\n",
              " 'discipline': 7693,\n",
              " 'lyoko': 52961,\n",
              " 'extend': 11310,\n",
              " 'nature': 873,\n",
              " \"'dickie'\": 52170,\n",
              " 'optimist': 40894,\n",
              " 'lapping': 30586,\n",
              " 'superficial': 3900,\n",
              " 'vestment': 52171,\n",
              " 'extent': 2823,\n",
              " 'tendons': 52172,\n",
              " \"heller's\": 52173,\n",
              " 'quagmires': 52174,\n",
              " 'miyako': 52175,\n",
              " 'moocow': 20601,\n",
              " \"coles'\": 52176,\n",
              " 'lookit': 40895,\n",
              " 'ravenously': 52177,\n",
              " 'levitating': 40896,\n",
              " 'perfunctorily': 52178,\n",
              " 'lookin': 30587,\n",
              " \"lot'\": 40898,\n",
              " 'lookie': 52179,\n",
              " 'fearlessly': 34870,\n",
              " 'libyan': 52181,\n",
              " 'fondles': 40899,\n",
              " 'gopher': 35714,\n",
              " 'wearying': 40901,\n",
              " \"nz's\": 52182,\n",
              " 'minuses': 27646,\n",
              " 'puposelessly': 52183,\n",
              " 'shandling': 52184,\n",
              " 'decapitates': 31268,\n",
              " 'humming': 11929,\n",
              " \"'nother\": 40902,\n",
              " 'smackdown': 21914,\n",
              " 'underdone': 30588,\n",
              " 'frf': 40903,\n",
              " 'triviality': 52185,\n",
              " 'fro': 25248,\n",
              " 'bothers': 8777,\n",
              " \"'kensington\": 52186,\n",
              " 'much': 73,\n",
              " 'muco': 34730,\n",
              " 'wiseguy': 22615,\n",
              " \"richie's\": 27648,\n",
              " 'tonino': 40904,\n",
              " 'unleavened': 52187,\n",
              " 'fry': 11587,\n",
              " \"'tv'\": 40905,\n",
              " 'toning': 40906,\n",
              " 'obese': 14361,\n",
              " 'sensationalized': 30589,\n",
              " 'spiv': 40907,\n",
              " 'spit': 6259,\n",
              " 'arkin': 7364,\n",
              " 'charleton': 21915,\n",
              " 'jeon': 16823,\n",
              " 'boardroom': 21916,\n",
              " 'doubts': 4989,\n",
              " 'spin': 3084,\n",
              " 'hepo': 53083,\n",
              " 'wildcat': 27649,\n",
              " 'venoms': 10584,\n",
              " 'misconstrues': 52191,\n",
              " 'mesmerising': 18514,\n",
              " 'misconstrued': 40908,\n",
              " 'rescinds': 52192,\n",
              " 'prostrate': 52193,\n",
              " 'majid': 40909,\n",
              " 'climbed': 16479,\n",
              " 'canoeing': 34731,\n",
              " 'majin': 52195,\n",
              " 'animie': 57804,\n",
              " 'sylke': 40910,\n",
              " 'conditioned': 14899,\n",
              " 'waddell': 40911,\n",
              " '3\\x85': 52196,\n",
              " 'hyperdrive': 41188,\n",
              " 'conditioner': 34732,\n",
              " 'bricklayer': 53153,\n",
              " 'hong': 2576,\n",
              " 'memoriam': 52198,\n",
              " 'inventively': 30592,\n",
              " \"levant's\": 25249,\n",
              " 'portobello': 20638,\n",
              " 'remand': 52200,\n",
              " 'mummified': 19504,\n",
              " 'honk': 27650,\n",
              " 'spews': 19505,\n",
              " 'visitations': 40912,\n",
              " 'mummifies': 52201,\n",
              " 'cavanaugh': 25250,\n",
              " 'zeon': 23385,\n",
              " \"jungle's\": 40913,\n",
              " 'viertel': 34733,\n",
              " 'frenchmen': 27651,\n",
              " 'torpedoes': 52202,\n",
              " 'schlessinger': 52203,\n",
              " 'torpedoed': 34734,\n",
              " 'blister': 69876,\n",
              " 'cinefest': 52204,\n",
              " 'furlough': 34735,\n",
              " 'mainsequence': 52205,\n",
              " 'mentors': 40914,\n",
              " 'academic': 9094,\n",
              " 'stillness': 20602,\n",
              " 'academia': 40915,\n",
              " 'lonelier': 52206,\n",
              " 'nibby': 52207,\n",
              " \"losers'\": 52208,\n",
              " 'cineastes': 40916,\n",
              " 'corporate': 4449,\n",
              " 'massaging': 40917,\n",
              " 'bellow': 30593,\n",
              " 'absurdities': 19506,\n",
              " 'expetations': 53241,\n",
              " 'nyfiken': 40918,\n",
              " 'mehras': 75638,\n",
              " 'lasse': 52209,\n",
              " 'visability': 52210,\n",
              " 'militarily': 33946,\n",
              " \"elder'\": 52211,\n",
              " 'gainsbourg': 19023,\n",
              " 'hah': 20603,\n",
              " 'hai': 13420,\n",
              " 'haj': 34736,\n",
              " 'hak': 25251,\n",
              " 'hal': 4311,\n",
              " 'ham': 4892,\n",
              " 'duffer': 53259,\n",
              " 'haa': 52213,\n",
              " 'had': 66,\n",
              " 'advancement': 11930,\n",
              " 'hag': 16825,\n",
              " \"hand'\": 25252,\n",
              " 'hay': 13421,\n",
              " 'mcnamara': 20604,\n",
              " \"mozart's\": 52214,\n",
              " 'duffel': 30731,\n",
              " 'haq': 30594,\n",
              " 'har': 13887,\n",
              " 'has': 44,\n",
              " 'hat': 2401,\n",
              " 'hav': 40919,\n",
              " 'haw': 30595,\n",
              " 'figtings': 52215,\n",
              " 'elders': 15495,\n",
              " 'underpanted': 52216,\n",
              " 'pninson': 52217,\n",
              " 'unequivocally': 27652,\n",
              " \"barbara's\": 23673,\n",
              " \"bello'\": 52219,\n",
              " 'indicative': 12997,\n",
              " 'yawnfest': 40920,\n",
              " 'hexploitation': 52220,\n",
              " \"loder's\": 52221,\n",
              " 'sleuthing': 27653,\n",
              " \"justin's\": 32622,\n",
              " \"'ball\": 52222,\n",
              " \"'summer\": 52223,\n",
              " \"'demons'\": 34935,\n",
              " \"mormon's\": 52225,\n",
              " \"laughton's\": 34737,\n",
              " 'debell': 52226,\n",
              " 'shipyard': 39724,\n",
              " 'unabashedly': 30597,\n",
              " 'disks': 40401,\n",
              " 'crowd': 2290,\n",
              " 'crowe': 10087,\n",
              " \"vancouver's\": 56434,\n",
              " 'mosques': 34738,\n",
              " 'crown': 6627,\n",
              " 'culpas': 52227,\n",
              " 'crows': 27654,\n",
              " 'surrell': 53344,\n",
              " 'flowless': 52229,\n",
              " 'sheirk': 52230,\n",
              " \"'three\": 40923,\n",
              " \"peterson'\": 52231,\n",
              " 'ooverall': 52232,\n",
              " 'perchance': 40924,\n",
              " 'bottom': 1321,\n",
              " 'chabert': 53363,\n",
              " 'sneha': 52233,\n",
              " 'inhuman': 13888,\n",
              " 'ichii': 52234,\n",
              " 'ursla': 52235,\n",
              " 'completly': 30598,\n",
              " 'moviedom': 40925,\n",
              " 'raddick': 52236,\n",
              " 'brundage': 51995,\n",
              " 'brigades': 40926,\n",
              " 'starring': 1181,\n",
              " \"'goal'\": 52237,\n",
              " 'caskets': 52238,\n",
              " 'willcock': 52239,\n",
              " \"threesome's\": 52240,\n",
              " \"mosque'\": 52241,\n",
              " \"cover's\": 52242,\n",
              " 'spaceships': 17637,\n",
              " 'anomalous': 40927,\n",
              " 'ptsd': 27655,\n",
              " 'shirdan': 52243,\n",
              " 'obscenity': 21962,\n",
              " 'lemmings': 30599,\n",
              " 'duccio': 30600,\n",
              " \"levene's\": 52244,\n",
              " \"'gorby'\": 52245,\n",
              " \"teenager's\": 25255,\n",
              " 'marshall': 5340,\n",
              " 'honeymoon': 9095,\n",
              " 'shoots': 3231,\n",
              " 'despised': 12258,\n",
              " 'okabasho': 52246,\n",
              " 'fabric': 8289,\n",
              " 'cannavale': 18515,\n",
              " 'raped': 3537,\n",
              " \"tutt's\": 52247,\n",
              " 'grasping': 17638,\n",
              " 'despises': 18516,\n",
              " \"thief's\": 40928,\n",
              " 'rapes': 8926,\n",
              " 'raper': 52248,\n",
              " \"eyre'\": 27656,\n",
              " 'walchek': 52249,\n",
              " \"elmo's\": 23386,\n",
              " 'perfumes': 40929,\n",
              " 'spurting': 21918,\n",
              " \"exposition'\\x85\": 52250,\n",
              " 'denoting': 52251,\n",
              " 'thesaurus': 34740,\n",
              " \"shoot'\": 40930,\n",
              " 'bonejack': 49759,\n",
              " 'simpsonian': 52253,\n",
              " 'hebetude': 30601,\n",
              " \"hallow's\": 34741,\n",
              " 'desperation\\x85': 52254,\n",
              " 'incinerator': 34742,\n",
              " 'congratulations': 10308,\n",
              " 'humbled': 52255,\n",
              " \"else's\": 5924,\n",
              " 'trelkovski': 40845,\n",
              " \"rape'\": 52256,\n",
              " \"'chapters'\": 59386,\n",
              " '1600s': 52257,\n",
              " 'martian': 7253,\n",
              " 'nicest': 25256,\n",
              " 'eyred': 52259,\n",
              " 'passenger': 9457,\n",
              " 'disgrace': 6041,\n",
              " 'moderne': 52260,\n",
              " 'barrymore': 5120,\n",
              " 'yankovich': 52261,\n",
              " 'moderns': 40931,\n",
              " 'studliest': 52262,\n",
              " 'bedsheet': 52263,\n",
              " 'decapitation': 14900,\n",
              " 'slurring': 52264,\n",
              " \"'nunsploitation'\": 52265,\n",
              " \"'character'\": 34743,\n",
              " 'cambodia': 9880,\n",
              " 'rebelious': 52266,\n",
              " 'pasadena': 27657,\n",
              " 'crowne': 40932,\n",
              " \"'bedchamber\": 52267,\n",
              " 'conjectural': 52268,\n",
              " 'appologize': 52269,\n",
              " 'halfassing': 52270,\n",
              " 'paycheque': 57816,\n",
              " 'palms': 20606,\n",
              " \"'islands\": 52271,\n",
              " 'hawked': 40933,\n",
              " 'palme': 21919,\n",
              " 'conservatively': 40934,\n",
              " 'larp': 64007,\n",
              " 'palma': 5558,\n",
              " 'smelling': 21920,\n",
              " 'aragorn': 12998,\n",
              " 'hawker': 52272,\n",
              " 'hawkes': 52273,\n",
              " 'explosions': 3975,\n",
              " 'loren': 8059,\n",
              " \"pyle's\": 52274,\n",
              " 'shootout': 6704,\n",
              " \"mike's\": 18517,\n",
              " \"driscoll's\": 52275,\n",
              " 'cogsworth': 40935,\n",
              " \"britian's\": 52276,\n",
              " 'childs': 34744,\n",
              " \"portrait's\": 52277,\n",
              " 'chain': 3626,\n",
              " 'whoever': 2497,\n",
              " 'puttered': 52278,\n",
              " 'childe': 52279,\n",
              " 'maywether': 52280,\n",
              " 'chair': 3036,\n",
              " \"rance's\": 52281,\n",
              " 'machu': 34745,\n",
              " 'ballet': 4517,\n",
              " 'grapples': 34746,\n",
              " 'summerize': 76152,\n",
              " 'freelance': 30603,\n",
              " \"andrea's\": 52283,\n",
              " '\\x91very': 52284,\n",
              " 'coolidge': 45879,\n",
              " 'mache': 18518,\n",
              " 'balled': 52285,\n",
              " 'grappled': 40937,\n",
              " 'macha': 18519,\n",
              " 'underlining': 21921,\n",
              " 'macho': 5623,\n",
              " 'oversight': 19507,\n",
              " 'machi': 25257,\n",
              " 'verbally': 11311,\n",
              " 'tenacious': 21922,\n",
              " 'windshields': 40938,\n",
              " 'paychecks': 18557,\n",
              " 'jerk': 3396,\n",
              " \"good'\": 11931,\n",
              " 'prancer': 34748,\n",
              " 'prances': 21923,\n",
              " 'olympus': 52286,\n",
              " 'lark': 21924,\n",
              " 'embark': 10785,\n",
              " 'gloomy': 7365,\n",
              " 'jehaan': 52287,\n",
              " 'turaqui': 52288,\n",
              " \"child'\": 20607,\n",
              " 'locked': 2894,\n",
              " 'pranced': 52289,\n",
              " 'exact': 2588,\n",
              " 'unattuned': 52290,\n",
              " 'minute': 783,\n",
              " 'skewed': 16118,\n",
              " 'hodgins': 40940,\n",
              " 'skewer': 34749,\n",
              " 'think\\x85': 52291,\n",
              " 'rosenstein': 38765,\n",
              " 'helmit': 52292,\n",
              " 'wrestlemanias': 34750,\n",
              " 'hindered': 16826,\n",
              " \"martha's\": 30604,\n",
              " 'cheree': 52293,\n",
              " \"pluckin'\": 52294,\n",
              " 'ogles': 40941,\n",
              " 'heavyweight': 11932,\n",
              " 'aada': 82190,\n",
              " 'chopping': 11312,\n",
              " 'strongboy': 61534,\n",
              " 'hegemonic': 41342,\n",
              " 'adorns': 40942,\n",
              " 'xxth': 41346,\n",
              " 'nobuhiro': 34751,\n",
              " 'capitães': 52298,\n",
              " 'kavogianni': 52299,\n",
              " 'antwerp': 13422,\n",
              " 'celebrated': 6538,\n",
              " 'roarke': 52300,\n",
              " 'baggins': 40943,\n",
              " 'cheeseburgers': 31270,\n",
              " 'matras': 52301,\n",
              " \"nineties'\": 52302,\n",
              " \"'craig'\": 52303,\n",
              " 'celebrates': 12999,\n",
              " 'unintentionally': 3383,\n",
              " 'drafted': 14362,\n",
              " 'climby': 52304,\n",
              " '303': 52305,\n",
              " 'oldies': 18520,\n",
              " 'climbs': 9096,\n",
              " 'honour': 9655,\n",
              " 'plucking': 34752,\n",
              " '305': 30074,\n",
              " 'address': 5514,\n",
              " 'menjou': 40944,\n",
              " \"'freak'\": 42592,\n",
              " 'dwindling': 19508,\n",
              " 'benson': 9458,\n",
              " 'white’s': 52307,\n",
              " 'shamelessness': 40945,\n",
              " 'impacted': 21925,\n",
              " 'upatz': 52308,\n",
              " 'cusack': 3840,\n",
              " \"flavia's\": 37567,\n",
              " 'effette': 52309,\n",
              " 'influx': 34753,\n",
              " 'boooooooo': 52310,\n",
              " 'dimitrova': 52311,\n",
              " 'houseman': 13423,\n",
              " 'bigas': 25259,\n",
              " 'boylen': 52312,\n",
              " 'phillipenes': 52313,\n",
              " 'fakery': 40946,\n",
              " \"grandpa's\": 27658,\n",
              " 'darnell': 27659,\n",
              " 'undergone': 19509,\n",
              " 'handbags': 52315,\n",
              " 'perished': 21926,\n",
              " 'pooped': 37778,\n",
              " 'vigour': 27660,\n",
              " 'opposed': 3627,\n",
              " 'etude': 52316,\n",
              " \"caine's\": 11799,\n",
              " 'doozers': 52317,\n",
              " 'photojournals': 34754,\n",
              " 'perishes': 52318,\n",
              " 'constrains': 34755,\n",
              " 'migenes': 40948,\n",
              " 'consoled': 30605,\n",
              " 'alastair': 16827,\n",
              " 'wvs': 52319,\n",
              " 'ooooooh': 52320,\n",
              " 'approving': 34756,\n",
              " 'consoles': 40949,\n",
              " 'disparagement': 52064,\n",
              " 'futureistic': 52322,\n",
              " 'rebounding': 52323,\n",
              " \"'date\": 52324,\n",
              " 'gregoire': 52325,\n",
              " 'rutherford': 21927,\n",
              " 'americanised': 34757,\n",
              " 'novikov': 82196,\n",
              " 'following': 1042,\n",
              " 'munroe': 34758,\n",
              " \"morita'\": 52326,\n",
              " 'christenssen': 52327,\n",
              " 'oatmeal': 23106,\n",
              " 'fossey': 25260,\n",
              " 'livered': 40950,\n",
              " 'listens': 13000,\n",
              " \"'marci\": 76164,\n",
              " \"otis's\": 52330,\n",
              " 'thanking': 23387,\n",
              " 'maude': 16019,\n",
              " 'extensions': 34759,\n",
              " 'ameteurish': 52332,\n",
              " \"commender's\": 52333,\n",
              " 'agricultural': 27661,\n",
              " 'convincingly': 4518,\n",
              " 'fueled': 17639,\n",
              " 'mahattan': 54014,\n",
              " \"paris's\": 40952,\n",
              " 'vulkan': 52336,\n",
              " 'stapes': 52337,\n",
              " 'odysessy': 52338,\n",
              " 'harmon': 12259,\n",
              " 'surfing': 4252,\n",
              " 'halloran': 23494,\n",
              " 'unbelieveably': 49580,\n",
              " \"'offed'\": 52339,\n",
              " 'quadrant': 30607,\n",
              " 'inhabiting': 19510,\n",
              " 'nebbish': 34760,\n",
              " 'forebears': 40953,\n",
              " 'skirmish': 34761,\n",
              " 'ocassionally': 52340,\n",
              " \"'resist\": 52341,\n",
              " 'impactful': 21928,\n",
              " 'spicier': 52342,\n",
              " 'touristy': 40954,\n",
              " \"'football'\": 52343,\n",
              " 'webpage': 40955,\n",
              " 'exurbia': 52345,\n",
              " 'jucier': 52346,\n",
              " 'professors': 14901,\n",
              " 'structuring': 34762,\n",
              " 'jig': 30608,\n",
              " 'overlord': 40956,\n",
              " 'disconnect': 25261,\n",
              " 'sniffle': 82201,\n",
              " 'slimeball': 40957,\n",
              " 'jia': 40958,\n",
              " 'milked': 16828,\n",
              " 'banjoes': 40959,\n",
              " 'jim': 1237,\n",
              " 'workforces': 52348,\n",
              " 'jip': 52349,\n",
              " 'rotweiller': 52350,\n",
              " 'mundaneness': 34763,\n",
              " \"'ninja'\": 52351,\n",
              " \"dead'\": 11040,\n",
              " \"cipriani's\": 40960,\n",
              " 'modestly': 20608,\n",
              " \"professor'\": 52352,\n",
              " 'shacked': 40961,\n",
              " 'bashful': 34764,\n",
              " 'sorter': 23388,\n",
              " 'overpowering': 16120,\n",
              " 'workmanlike': 18521,\n",
              " 'henpecked': 27662,\n",
              " 'sorted': 18522,\n",
              " \"jōb's\": 52354,\n",
              " \"'always\": 52355,\n",
              " \"'baptists\": 34765,\n",
              " 'dreamcatchers': 52356,\n",
              " \"'silence'\": 52357,\n",
              " 'hickory': 21929,\n",
              " 'fun\\x97yet': 52358,\n",
              " 'breakumentary': 52359,\n",
              " 'didn': 15496,\n",
              " 'didi': 52360,\n",
              " 'pealing': 52361,\n",
              " 'dispite': 40962,\n",
              " \"italy's\": 25262,\n",
              " 'instability': 21930,\n",
              " 'quarter': 6539,\n",
              " 'quartet': 12608,\n",
              " 'padmé': 52362,\n",
              " \"'bleedmedry\": 52363,\n",
              " 'pahalniuk': 52364,\n",
              " 'honduras': 52365,\n",
              " 'bursting': 10786,\n",
              " \"pablo's\": 41465,\n",
              " 'irremediably': 52367,\n",
              " 'presages': 40963,\n",
              " 'bowlegged': 57832,\n",
              " 'dalip': 65183,\n",
              " 'entering': 6260,\n",
              " 'newsradio': 76172,\n",
              " 'presaged': 54150,\n",
              " \"giallo's\": 27663,\n",
              " 'bouyant': 40964,\n",
              " 'amerterish': 52368,\n",
              " 'rajni': 18523,\n",
              " 'leeves': 30610,\n",
              " 'macauley': 34767,\n",
              " 'seriously': 612,\n",
              " 'sugercoma': 52369,\n",
              " 'grimstead': 52370,\n",
              " \"'fairy'\": 52371,\n",
              " 'zenda': 30611,\n",
              " \"'twins'\": 52372,\n",
              " 'realisation': 17640,\n",
              " 'highsmith': 27664,\n",
              " 'raunchy': 7817,\n",
              " 'incentives': 40965,\n",
              " 'flatson': 52374,\n",
              " 'snooker': 35097,\n",
              " 'crazies': 16829,\n",
              " 'crazier': 14902,\n",
              " 'grandma': 7094,\n",
              " 'napunsaktha': 52375,\n",
              " 'workmanship': 30612,\n",
              " 'reisner': 52376,\n",
              " \"sanford's\": 61306,\n",
              " '\\x91doña': 52377,\n",
              " 'modest': 6108,\n",
              " \"everything's\": 19153,\n",
              " 'hamer': 40966,\n",
              " \"couldn't'\": 52379,\n",
              " 'quibble': 13001,\n",
              " 'socking': 52380,\n",
              " 'tingler': 21931,\n",
              " 'gutman': 52381,\n",
              " 'lachlan': 40967,\n",
              " 'tableaus': 52382,\n",
              " 'headbanger': 52383,\n",
              " 'spoken': 2847,\n",
              " 'cerebrally': 34768,\n",
              " \"'road\": 23490,\n",
              " 'tableaux': 21932,\n",
              " \"proust's\": 40968,\n",
              " 'periodical': 40969,\n",
              " \"shoveller's\": 52385,\n",
              " 'tamara': 25263,\n",
              " 'affords': 17641,\n",
              " 'concert': 3249,\n",
              " \"yara's\": 87955,\n",
              " 'someome': 52386,\n",
              " 'lingering': 8424,\n",
              " \"abraham's\": 41511,\n",
              " 'beesley': 34769,\n",
              " 'cherbourg': 34770,\n",
              " 'kagan': 28624,\n",
              " 'snatch': 9097,\n",
              " \"miyazaki's\": 9260,\n",
              " 'absorbs': 25264,\n",
              " \"koltai's\": 40970,\n",
              " 'tingled': 64027,\n",
              " 'crossroads': 19511,\n",
              " 'rehab': 16121,\n",
              " 'falworth': 52389,\n",
              " 'sequals': 52390,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojoDHJnEqorm"
      },
      "source": [
        "###reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])  #(key, value)를 word_index.items에서 받아가지고 (value, key)를 거꾸로 한다음에 \n",
        "#꺼꾸로 해준 리스트를 나중에는 dit해서 딕셔너리로 바꾸어 준다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HFpYnF6rJI0",
        "outputId": "b811e8b6-88fc-46ff-be3c-1947e71ac11e"
      },
      "source": [
        "###[(value, key) for (key, value) in word_index.items()] #이대로 하게 되면 리스트 형식이 만들어지게 된다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(34701, 'fawn'),\n",
              " (52006, 'tsukino'),\n",
              " (52007, 'nunnery'),\n",
              " (16816, 'sonja'),\n",
              " (63951, 'vani'),\n",
              " (1408, 'woods'),\n",
              " (16115, 'spiders'),\n",
              " (2345, 'hanging'),\n",
              " (2289, 'woody'),\n",
              " (52008, 'trawling'),\n",
              " (52009, \"hold's\"),\n",
              " (11307, 'comically'),\n",
              " (40830, 'localized'),\n",
              " (30568, 'disobeying'),\n",
              " (52010, \"'royale\"),\n",
              " (40831, \"harpo's\"),\n",
              " (52011, 'canet'),\n",
              " (19313, 'aileen'),\n",
              " (52012, 'acurately'),\n",
              " (52013, \"diplomat's\"),\n",
              " (25242, 'rickman'),\n",
              " (6746, 'arranged'),\n",
              " (52014, 'rumbustious'),\n",
              " (52015, 'familiarness'),\n",
              " (52016, \"spider'\"),\n",
              " (68804, 'hahahah'),\n",
              " (52017, \"wood'\"),\n",
              " (40833, 'transvestism'),\n",
              " (34702, \"hangin'\"),\n",
              " (2338, 'bringing'),\n",
              " (40834, 'seamier'),\n",
              " (34703, 'wooded'),\n",
              " (52018, 'bravora'),\n",
              " (16817, 'grueling'),\n",
              " (1636, 'wooden'),\n",
              " (16818, 'wednesday'),\n",
              " (52019, \"'prix\"),\n",
              " (34704, 'altagracia'),\n",
              " (52020, 'circuitry'),\n",
              " (11585, 'crotch'),\n",
              " (57766, 'busybody'),\n",
              " (52021, \"tart'n'tangy\"),\n",
              " (14129, 'burgade'),\n",
              " (52023, 'thrace'),\n",
              " (11038, \"tom's\"),\n",
              " (52025, 'snuggles'),\n",
              " (29114, 'francesco'),\n",
              " (52027, 'complainers'),\n",
              " (52125, 'templarios'),\n",
              " (40835, '272'),\n",
              " (52028, '273'),\n",
              " (52130, 'zaniacs'),\n",
              " (34706, '275'),\n",
              " (27631, 'consenting'),\n",
              " (40836, 'snuggled'),\n",
              " (15492, 'inanimate'),\n",
              " (52030, 'uality'),\n",
              " (11926, 'bronte'),\n",
              " (4010, 'errors'),\n",
              " (3230, 'dialogs'),\n",
              " (52031, \"yomada's\"),\n",
              " (34707, \"madman's\"),\n",
              " (30585, 'dialoge'),\n",
              " (52033, 'usenet'),\n",
              " (40837, 'videodrome'),\n",
              " (26338, \"kid'\"),\n",
              " (52034, 'pawed'),\n",
              " (30569, \"'girlfriend'\"),\n",
              " (52035, \"'pleasure\"),\n",
              " (52036, \"'reloaded'\"),\n",
              " (40839, \"kazakos'\"),\n",
              " (52037, 'rocque'),\n",
              " (52038, 'mailings'),\n",
              " (11927, 'brainwashed'),\n",
              " (16819, 'mcanally'),\n",
              " (52039, \"tom''\"),\n",
              " (25243, 'kurupt'),\n",
              " (21905, 'affiliated'),\n",
              " (52040, 'babaganoosh'),\n",
              " (40840, \"noe's\"),\n",
              " (40841, 'quart'),\n",
              " (359, 'kids'),\n",
              " (5034, 'uplifting'),\n",
              " (7093, 'controversy'),\n",
              " (21906, 'kida'),\n",
              " (23379, 'kidd'),\n",
              " (52041, \"error'\"),\n",
              " (52042, 'neurologist'),\n",
              " (18510, 'spotty'),\n",
              " (30570, 'cobblers'),\n",
              " (9878, 'projection'),\n",
              " (40842, 'fastforwarding'),\n",
              " (52043, 'sters'),\n",
              " (52044, \"eggar's\"),\n",
              " (52045, 'etherything'),\n",
              " (40843, 'gateshead'),\n",
              " (34708, 'airball'),\n",
              " (25244, 'unsinkable'),\n",
              " (7180, 'stern'),\n",
              " (52046, \"cervi's\"),\n",
              " (40844, 'dnd'),\n",
              " (11586, 'dna'),\n",
              " (20598, 'insecurity'),\n",
              " (52047, \"'reboot'\"),\n",
              " (11037, 'trelkovsky'),\n",
              " (52048, 'jaekel'),\n",
              " (52049, 'sidebars'),\n",
              " (52050, \"sforza's\"),\n",
              " (17633, 'distortions'),\n",
              " (52051, 'mutinies'),\n",
              " (30602, 'sermons'),\n",
              " (40846, '7ft'),\n",
              " (52052, 'boobage'),\n",
              " (52053, \"o'bannon's\"),\n",
              " (23380, 'populations'),\n",
              " (52054, 'chulak'),\n",
              " (27633, 'mesmerize'),\n",
              " (52055, 'quinnell'),\n",
              " (10307, 'yahoo'),\n",
              " (52057, 'meteorologist'),\n",
              " (42577, 'beswick'),\n",
              " (15493, 'boorman'),\n",
              " (40847, 'voicework'),\n",
              " (52058, \"ster'\"),\n",
              " (22922, 'blustering'),\n",
              " (52059, 'hj'),\n",
              " (27634, 'intake'),\n",
              " (5621, 'morally'),\n",
              " (40849, 'jumbling'),\n",
              " (52060, 'bowersock'),\n",
              " (52061, \"'porky's'\"),\n",
              " (16821, 'gershon'),\n",
              " (40850, 'ludicrosity'),\n",
              " (52062, 'coprophilia'),\n",
              " (40851, 'expressively'),\n",
              " (19500, \"india's\"),\n",
              " (34710, \"post's\"),\n",
              " (52063, 'wana'),\n",
              " (5283, 'wang'),\n",
              " (30571, 'wand'),\n",
              " (25245, 'wane'),\n",
              " (52321, 'edgeways'),\n",
              " (34711, 'titanium'),\n",
              " (40852, 'pinta'),\n",
              " (178, 'want'),\n",
              " (30572, 'pinto'),\n",
              " (52065, 'whoopdedoodles'),\n",
              " (21908, 'tchaikovsky'),\n",
              " (2103, 'travel'),\n",
              " (52066, \"'victory'\"),\n",
              " (11928, 'copious'),\n",
              " (22433, 'gouge'),\n",
              " (52067, \"chapters'\"),\n",
              " (6702, 'barbra'),\n",
              " (30573, 'uselessness'),\n",
              " (52068, \"wan'\"),\n",
              " (27635, 'assimilated'),\n",
              " (16116, 'petiot'),\n",
              " (52069, 'most\\x85and'),\n",
              " (3930, 'dinosaurs'),\n",
              " (352, 'wrong'),\n",
              " (52070, 'seda'),\n",
              " (52071, 'stollen'),\n",
              " (34712, 'sentencing'),\n",
              " (40853, 'ouroboros'),\n",
              " (40854, 'assimilates'),\n",
              " (40855, 'colorfully'),\n",
              " (27636, 'glenne'),\n",
              " (52072, 'dongen'),\n",
              " (4760, 'subplots'),\n",
              " (52073, 'kiloton'),\n",
              " (23381, 'chandon'),\n",
              " (34713, \"effect'\"),\n",
              " (27637, 'snugly'),\n",
              " (40856, 'kuei'),\n",
              " (9092, 'welcomed'),\n",
              " (30071, 'dishonor'),\n",
              " (52075, 'concurrence'),\n",
              " (23382, 'stoicism'),\n",
              " (14896, \"guys'\"),\n",
              " (52077, \"beroemd'\"),\n",
              " (6703, 'butcher'),\n",
              " (40857, \"melfi's\"),\n",
              " (30623, 'aargh'),\n",
              " (20599, 'playhouse'),\n",
              " (11308, 'wickedly'),\n",
              " (1180, 'fit'),\n",
              " (52078, 'labratory'),\n",
              " (40859, 'lifeline'),\n",
              " (1927, 'screaming'),\n",
              " (4287, 'fix'),\n",
              " (52079, 'cineliterate'),\n",
              " (52080, 'fic'),\n",
              " (52081, 'fia'),\n",
              " (34714, 'fig'),\n",
              " (52082, 'fmvs'),\n",
              " (52083, 'fie'),\n",
              " (52084, 'reentered'),\n",
              " (30574, 'fin'),\n",
              " (52085, 'doctresses'),\n",
              " (52086, 'fil'),\n",
              " (12606, 'zucker'),\n",
              " (31931, 'ached'),\n",
              " (52088, 'counsil'),\n",
              " (52089, 'paterfamilias'),\n",
              " (13885, 'songwriter'),\n",
              " (34715, 'shivam'),\n",
              " (9654, 'hurting'),\n",
              " (299, 'effects'),\n",
              " (52090, 'slauther'),\n",
              " (52091, \"'flame'\"),\n",
              " (52092, 'sommerset'),\n",
              " (52093, 'interwhined'),\n",
              " (27638, 'whacking'),\n",
              " (52094, 'bartok'),\n",
              " (8775, 'barton'),\n",
              " (21909, 'frewer'),\n",
              " (52095, \"fi'\"),\n",
              " (6192, 'ingrid'),\n",
              " (30575, 'stribor'),\n",
              " (52096, 'approporiately'),\n",
              " (52097, 'wobblyhand'),\n",
              " (52098, 'tantalisingly'),\n",
              " (52099, 'ankylosaurus'),\n",
              " (17634, 'parasites'),\n",
              " (52100, 'childen'),\n",
              " (52101, \"jenkins'\"),\n",
              " (52102, 'metafiction'),\n",
              " (17635, 'golem'),\n",
              " (40860, 'indiscretion'),\n",
              " (23383, \"reeves'\"),\n",
              " (57781, \"inamorata's\"),\n",
              " (52104, 'brittannica'),\n",
              " (7916, 'adapt'),\n",
              " (30576, \"russo's\"),\n",
              " (48246, 'guitarists'),\n",
              " (10553, 'abbott'),\n",
              " (40861, 'abbots'),\n",
              " (17649, 'lanisha'),\n",
              " (40863, 'magickal'),\n",
              " (52105, 'mattter'),\n",
              " (52106, \"'willy\"),\n",
              " (34716, 'pumpkins'),\n",
              " (52107, 'stuntpeople'),\n",
              " (30577, 'estimate'),\n",
              " (40864, 'ugghhh'),\n",
              " (11309, 'gameplay'),\n",
              " (52108, \"wern't\"),\n",
              " (40865, \"n'sync\"),\n",
              " (16117, 'sickeningly'),\n",
              " (40866, 'chiara'),\n",
              " (4011, 'disturbed'),\n",
              " (40867, 'portmanteau'),\n",
              " (52109, 'ineffectively'),\n",
              " (82143, \"duchonvey's\"),\n",
              " (37519, \"nasty'\"),\n",
              " (1285, 'purpose'),\n",
              " (52112, 'lazers'),\n",
              " (28105, 'lightened'),\n",
              " (52113, 'kaliganj'),\n",
              " (52114, 'popularism'),\n",
              " (18511, \"damme's\"),\n",
              " (30578, 'stylistics'),\n",
              " (52115, 'mindgaming'),\n",
              " (46449, 'spoilerish'),\n",
              " (52117, \"'corny'\"),\n",
              " (34718, 'boerner'),\n",
              " (6792, 'olds'),\n",
              " (52118, 'bakelite'),\n",
              " (27639, 'renovated'),\n",
              " (27640, 'forrester'),\n",
              " (52119, \"lumiere's\"),\n",
              " (52024, 'gaskets'),\n",
              " (884, 'needed'),\n",
              " (34719, 'smight'),\n",
              " (1297, 'master'),\n",
              " (25905, \"edie's\"),\n",
              " (40868, 'seeber'),\n",
              " (52120, 'hiya'),\n",
              " (52121, 'fuzziness'),\n",
              " (14897, 'genesis'),\n",
              " (12607, 'rewards'),\n",
              " (30579, 'enthrall'),\n",
              " (40869, \"'about\"),\n",
              " (52122, \"recollection's\"),\n",
              " (11039, 'mutilated'),\n",
              " (52123, 'fatherlands'),\n",
              " (52124, \"fischer's\"),\n",
              " (5399, 'positively'),\n",
              " (34705, '270'),\n",
              " (34720, 'ahmed'),\n",
              " (9836, 'zatoichi'),\n",
              " (13886, 'bannister'),\n",
              " (52127, 'anniversaries'),\n",
              " (30580, \"helm's\"),\n",
              " (52128, \"'work'\"),\n",
              " (34721, 'exclaimed'),\n",
              " (52129, \"'unfunny'\"),\n",
              " (52029, '274'),\n",
              " (544, 'feeling'),\n",
              " (52131, \"wanda's\"),\n",
              " (33266, 'dolan'),\n",
              " (52133, '278'),\n",
              " (52134, 'peacoat'),\n",
              " (40870, 'brawny'),\n",
              " (40871, 'mishra'),\n",
              " (40872, 'worlders'),\n",
              " (52135, 'protags'),\n",
              " (52136, 'skullcap'),\n",
              " (57596, 'dastagir'),\n",
              " (5622, 'affairs'),\n",
              " (7799, 'wholesome'),\n",
              " (52137, 'hymen'),\n",
              " (25246, 'paramedics'),\n",
              " (52138, 'unpersons'),\n",
              " (52139, 'heavyarms'),\n",
              " (52140, 'affaire'),\n",
              " (52141, 'coulisses'),\n",
              " (40873, 'hymer'),\n",
              " (52142, 'kremlin'),\n",
              " (30581, 'shipments'),\n",
              " (52143, 'pixilated'),\n",
              " (30582, \"'00s\"),\n",
              " (18512, 'diminishing'),\n",
              " (1357, 'cinematic'),\n",
              " (14898, 'resonates'),\n",
              " (40874, 'simplify'),\n",
              " (40875, \"nature'\"),\n",
              " (40876, 'temptresses'),\n",
              " (16822, 'reverence'),\n",
              " (19502, 'resonated'),\n",
              " (34722, 'dailey'),\n",
              " (52144, '2\\x85'),\n",
              " (27641, 'treize'),\n",
              " (52145, 'majo'),\n",
              " (21910, 'kiya'),\n",
              " (52146, 'woolnough'),\n",
              " (39797, 'thanatos'),\n",
              " (35731, 'sandoval'),\n",
              " (40879, 'dorama'),\n",
              " (52147, \"o'shaughnessy\"),\n",
              " (4988, 'tech'),\n",
              " (32018, 'fugitives'),\n",
              " (30583, 'teck'),\n",
              " (76125, \"'e'\"),\n",
              " (40881, 'doesn’t'),\n",
              " (52149, 'purged'),\n",
              " (657, 'saying'),\n",
              " (41095, \"martians'\"),\n",
              " (23418, 'norliss'),\n",
              " (27642, 'dickey'),\n",
              " (52152, 'dicker'),\n",
              " (52153, \"'sependipity\"),\n",
              " (8422, 'padded'),\n",
              " (57792, 'ordell'),\n",
              " (40882, \"sturges'\"),\n",
              " (52154, 'independentcritics'),\n",
              " (5745, 'tempted'),\n",
              " (34724, \"atkinson's\"),\n",
              " (25247, 'hounded'),\n",
              " (52155, 'apace'),\n",
              " (15494, 'clicked'),\n",
              " (30584, \"'humor'\"),\n",
              " (17177, \"martino's\"),\n",
              " (52156, \"'supporting\"),\n",
              " (52032, 'warmongering'),\n",
              " (34725, \"zemeckis's\"),\n",
              " (21911, 'lube'),\n",
              " (52157, 'shocky'),\n",
              " (7476, 'plate'),\n",
              " (40883, 'plata'),\n",
              " (40884, 'sturgess'),\n",
              " (40885, \"nerds'\"),\n",
              " (20600, 'plato'),\n",
              " (34726, 'plath'),\n",
              " (40886, 'platt'),\n",
              " (52159, 'mcnab'),\n",
              " (27643, 'clumsiness'),\n",
              " (3899, 'altogether'),\n",
              " (42584, 'massacring'),\n",
              " (52160, 'bicenntinial'),\n",
              " (40887, 'skaal'),\n",
              " (14360, 'droning'),\n",
              " (8776, 'lds'),\n",
              " (21912, 'jaguar'),\n",
              " (34727, \"cale's\"),\n",
              " (1777, 'nicely'),\n",
              " (4588, 'mummy'),\n",
              " (18513, \"lot's\"),\n",
              " (10086, 'patch'),\n",
              " (50202, 'kerkhof'),\n",
              " (52161, \"leader's\"),\n",
              " (27644, \"'movie\"),\n",
              " (52162, 'uncomfirmed'),\n",
              " (40888, 'heirloom'),\n",
              " (47360, 'wrangle'),\n",
              " (52163, 'emotion\\x85'),\n",
              " (52164, \"'stargate'\"),\n",
              " (40889, 'pinoy'),\n",
              " (40890, 'conchatta'),\n",
              " (41128, 'broeke'),\n",
              " (40891, 'advisedly'),\n",
              " (17636, \"barker's\"),\n",
              " (52166, 'descours'),\n",
              " (772, 'lots'),\n",
              " (9259, 'lotr'),\n",
              " (9879, 'irs'),\n",
              " (52167, 'lott'),\n",
              " (40892, 'xvi'),\n",
              " (34728, 'irk'),\n",
              " (52168, 'irl'),\n",
              " (6887, 'ira'),\n",
              " (21913, 'belzer'),\n",
              " (52169, 'irc'),\n",
              " (27645, 'ire'),\n",
              " (40893, 'requisites'),\n",
              " (7693, 'discipline'),\n",
              " (52961, 'lyoko'),\n",
              " (11310, 'extend'),\n",
              " (873, 'nature'),\n",
              " (52170, \"'dickie'\"),\n",
              " (40894, 'optimist'),\n",
              " (30586, 'lapping'),\n",
              " (3900, 'superficial'),\n",
              " (52171, 'vestment'),\n",
              " (2823, 'extent'),\n",
              " (52172, 'tendons'),\n",
              " (52173, \"heller's\"),\n",
              " (52174, 'quagmires'),\n",
              " (52175, 'miyako'),\n",
              " (20601, 'moocow'),\n",
              " (52176, \"coles'\"),\n",
              " (40895, 'lookit'),\n",
              " (52177, 'ravenously'),\n",
              " (40896, 'levitating'),\n",
              " (52178, 'perfunctorily'),\n",
              " (30587, 'lookin'),\n",
              " (40898, \"lot'\"),\n",
              " (52179, 'lookie'),\n",
              " (34870, 'fearlessly'),\n",
              " (52181, 'libyan'),\n",
              " (40899, 'fondles'),\n",
              " (35714, 'gopher'),\n",
              " (40901, 'wearying'),\n",
              " (52182, \"nz's\"),\n",
              " (27646, 'minuses'),\n",
              " (52183, 'puposelessly'),\n",
              " (52184, 'shandling'),\n",
              " (31268, 'decapitates'),\n",
              " (11929, 'humming'),\n",
              " (40902, \"'nother\"),\n",
              " (21914, 'smackdown'),\n",
              " (30588, 'underdone'),\n",
              " (40903, 'frf'),\n",
              " (52185, 'triviality'),\n",
              " (25248, 'fro'),\n",
              " (8777, 'bothers'),\n",
              " (52186, \"'kensington\"),\n",
              " (73, 'much'),\n",
              " (34730, 'muco'),\n",
              " (22615, 'wiseguy'),\n",
              " (27648, \"richie's\"),\n",
              " (40904, 'tonino'),\n",
              " (52187, 'unleavened'),\n",
              " (11587, 'fry'),\n",
              " (40905, \"'tv'\"),\n",
              " (40906, 'toning'),\n",
              " (14361, 'obese'),\n",
              " (30589, 'sensationalized'),\n",
              " (40907, 'spiv'),\n",
              " (6259, 'spit'),\n",
              " (7364, 'arkin'),\n",
              " (21915, 'charleton'),\n",
              " (16823, 'jeon'),\n",
              " (21916, 'boardroom'),\n",
              " (4989, 'doubts'),\n",
              " (3084, 'spin'),\n",
              " (53083, 'hepo'),\n",
              " (27649, 'wildcat'),\n",
              " (10584, 'venoms'),\n",
              " (52191, 'misconstrues'),\n",
              " (18514, 'mesmerising'),\n",
              " (40908, 'misconstrued'),\n",
              " (52192, 'rescinds'),\n",
              " (52193, 'prostrate'),\n",
              " (40909, 'majid'),\n",
              " (16479, 'climbed'),\n",
              " (34731, 'canoeing'),\n",
              " (52195, 'majin'),\n",
              " (57804, 'animie'),\n",
              " (40910, 'sylke'),\n",
              " (14899, 'conditioned'),\n",
              " (40911, 'waddell'),\n",
              " (52196, '3\\x85'),\n",
              " (41188, 'hyperdrive'),\n",
              " (34732, 'conditioner'),\n",
              " (53153, 'bricklayer'),\n",
              " (2576, 'hong'),\n",
              " (52198, 'memoriam'),\n",
              " (30592, 'inventively'),\n",
              " (25249, \"levant's\"),\n",
              " (20638, 'portobello'),\n",
              " (52200, 'remand'),\n",
              " (19504, 'mummified'),\n",
              " (27650, 'honk'),\n",
              " (19505, 'spews'),\n",
              " (40912, 'visitations'),\n",
              " (52201, 'mummifies'),\n",
              " (25250, 'cavanaugh'),\n",
              " (23385, 'zeon'),\n",
              " (40913, \"jungle's\"),\n",
              " (34733, 'viertel'),\n",
              " (27651, 'frenchmen'),\n",
              " (52202, 'torpedoes'),\n",
              " (52203, 'schlessinger'),\n",
              " (34734, 'torpedoed'),\n",
              " (69876, 'blister'),\n",
              " (52204, 'cinefest'),\n",
              " (34735, 'furlough'),\n",
              " (52205, 'mainsequence'),\n",
              " (40914, 'mentors'),\n",
              " (9094, 'academic'),\n",
              " (20602, 'stillness'),\n",
              " (40915, 'academia'),\n",
              " (52206, 'lonelier'),\n",
              " (52207, 'nibby'),\n",
              " (52208, \"losers'\"),\n",
              " (40916, 'cineastes'),\n",
              " (4449, 'corporate'),\n",
              " (40917, 'massaging'),\n",
              " (30593, 'bellow'),\n",
              " (19506, 'absurdities'),\n",
              " (53241, 'expetations'),\n",
              " (40918, 'nyfiken'),\n",
              " (75638, 'mehras'),\n",
              " (52209, 'lasse'),\n",
              " (52210, 'visability'),\n",
              " (33946, 'militarily'),\n",
              " (52211, \"elder'\"),\n",
              " (19023, 'gainsbourg'),\n",
              " (20603, 'hah'),\n",
              " (13420, 'hai'),\n",
              " (34736, 'haj'),\n",
              " (25251, 'hak'),\n",
              " (4311, 'hal'),\n",
              " (4892, 'ham'),\n",
              " (53259, 'duffer'),\n",
              " (52213, 'haa'),\n",
              " (66, 'had'),\n",
              " (11930, 'advancement'),\n",
              " (16825, 'hag'),\n",
              " (25252, \"hand'\"),\n",
              " (13421, 'hay'),\n",
              " (20604, 'mcnamara'),\n",
              " (52214, \"mozart's\"),\n",
              " (30731, 'duffel'),\n",
              " (30594, 'haq'),\n",
              " (13887, 'har'),\n",
              " (44, 'has'),\n",
              " (2401, 'hat'),\n",
              " (40919, 'hav'),\n",
              " (30595, 'haw'),\n",
              " (52215, 'figtings'),\n",
              " (15495, 'elders'),\n",
              " (52216, 'underpanted'),\n",
              " (52217, 'pninson'),\n",
              " (27652, 'unequivocally'),\n",
              " (23673, \"barbara's\"),\n",
              " (52219, \"bello'\"),\n",
              " (12997, 'indicative'),\n",
              " (40920, 'yawnfest'),\n",
              " (52220, 'hexploitation'),\n",
              " (52221, \"loder's\"),\n",
              " (27653, 'sleuthing'),\n",
              " (32622, \"justin's\"),\n",
              " (52222, \"'ball\"),\n",
              " (52223, \"'summer\"),\n",
              " (34935, \"'demons'\"),\n",
              " (52225, \"mormon's\"),\n",
              " (34737, \"laughton's\"),\n",
              " (52226, 'debell'),\n",
              " (39724, 'shipyard'),\n",
              " (30597, 'unabashedly'),\n",
              " (40401, 'disks'),\n",
              " (2290, 'crowd'),\n",
              " (10087, 'crowe'),\n",
              " (56434, \"vancouver's\"),\n",
              " (34738, 'mosques'),\n",
              " (6627, 'crown'),\n",
              " (52227, 'culpas'),\n",
              " (27654, 'crows'),\n",
              " (53344, 'surrell'),\n",
              " (52229, 'flowless'),\n",
              " (52230, 'sheirk'),\n",
              " (40923, \"'three\"),\n",
              " (52231, \"peterson'\"),\n",
              " (52232, 'ooverall'),\n",
              " (40924, 'perchance'),\n",
              " (1321, 'bottom'),\n",
              " (53363, 'chabert'),\n",
              " (52233, 'sneha'),\n",
              " (13888, 'inhuman'),\n",
              " (52234, 'ichii'),\n",
              " (52235, 'ursla'),\n",
              " (30598, 'completly'),\n",
              " (40925, 'moviedom'),\n",
              " (52236, 'raddick'),\n",
              " (51995, 'brundage'),\n",
              " (40926, 'brigades'),\n",
              " (1181, 'starring'),\n",
              " (52237, \"'goal'\"),\n",
              " (52238, 'caskets'),\n",
              " (52239, 'willcock'),\n",
              " (52240, \"threesome's\"),\n",
              " (52241, \"mosque'\"),\n",
              " (52242, \"cover's\"),\n",
              " (17637, 'spaceships'),\n",
              " (40927, 'anomalous'),\n",
              " (27655, 'ptsd'),\n",
              " (52243, 'shirdan'),\n",
              " (21962, 'obscenity'),\n",
              " (30599, 'lemmings'),\n",
              " (30600, 'duccio'),\n",
              " (52244, \"levene's\"),\n",
              " (52245, \"'gorby'\"),\n",
              " (25255, \"teenager's\"),\n",
              " (5340, 'marshall'),\n",
              " (9095, 'honeymoon'),\n",
              " (3231, 'shoots'),\n",
              " (12258, 'despised'),\n",
              " (52246, 'okabasho'),\n",
              " (8289, 'fabric'),\n",
              " (18515, 'cannavale'),\n",
              " (3537, 'raped'),\n",
              " (52247, \"tutt's\"),\n",
              " (17638, 'grasping'),\n",
              " (18516, 'despises'),\n",
              " (40928, \"thief's\"),\n",
              " (8926, 'rapes'),\n",
              " (52248, 'raper'),\n",
              " (27656, \"eyre'\"),\n",
              " (52249, 'walchek'),\n",
              " (23386, \"elmo's\"),\n",
              " (40929, 'perfumes'),\n",
              " (21918, 'spurting'),\n",
              " (52250, \"exposition'\\x85\"),\n",
              " (52251, 'denoting'),\n",
              " (34740, 'thesaurus'),\n",
              " (40930, \"shoot'\"),\n",
              " (49759, 'bonejack'),\n",
              " (52253, 'simpsonian'),\n",
              " (30601, 'hebetude'),\n",
              " (34741, \"hallow's\"),\n",
              " (52254, 'desperation\\x85'),\n",
              " (34742, 'incinerator'),\n",
              " (10308, 'congratulations'),\n",
              " (52255, 'humbled'),\n",
              " (5924, \"else's\"),\n",
              " (40845, 'trelkovski'),\n",
              " (52256, \"rape'\"),\n",
              " (59386, \"'chapters'\"),\n",
              " (52257, '1600s'),\n",
              " (7253, 'martian'),\n",
              " (25256, 'nicest'),\n",
              " (52259, 'eyred'),\n",
              " (9457, 'passenger'),\n",
              " (6041, 'disgrace'),\n",
              " (52260, 'moderne'),\n",
              " (5120, 'barrymore'),\n",
              " (52261, 'yankovich'),\n",
              " (40931, 'moderns'),\n",
              " (52262, 'studliest'),\n",
              " (52263, 'bedsheet'),\n",
              " (14900, 'decapitation'),\n",
              " (52264, 'slurring'),\n",
              " (52265, \"'nunsploitation'\"),\n",
              " (34743, \"'character'\"),\n",
              " (9880, 'cambodia'),\n",
              " (52266, 'rebelious'),\n",
              " (27657, 'pasadena'),\n",
              " (40932, 'crowne'),\n",
              " (52267, \"'bedchamber\"),\n",
              " (52268, 'conjectural'),\n",
              " (52269, 'appologize'),\n",
              " (52270, 'halfassing'),\n",
              " (57816, 'paycheque'),\n",
              " (20606, 'palms'),\n",
              " (52271, \"'islands\"),\n",
              " (40933, 'hawked'),\n",
              " (21919, 'palme'),\n",
              " (40934, 'conservatively'),\n",
              " (64007, 'larp'),\n",
              " (5558, 'palma'),\n",
              " (21920, 'smelling'),\n",
              " (12998, 'aragorn'),\n",
              " (52272, 'hawker'),\n",
              " (52273, 'hawkes'),\n",
              " (3975, 'explosions'),\n",
              " (8059, 'loren'),\n",
              " (52274, \"pyle's\"),\n",
              " (6704, 'shootout'),\n",
              " (18517, \"mike's\"),\n",
              " (52275, \"driscoll's\"),\n",
              " (40935, 'cogsworth'),\n",
              " (52276, \"britian's\"),\n",
              " (34744, 'childs'),\n",
              " (52277, \"portrait's\"),\n",
              " (3626, 'chain'),\n",
              " (2497, 'whoever'),\n",
              " (52278, 'puttered'),\n",
              " (52279, 'childe'),\n",
              " (52280, 'maywether'),\n",
              " (3036, 'chair'),\n",
              " (52281, \"rance's\"),\n",
              " (34745, 'machu'),\n",
              " (4517, 'ballet'),\n",
              " (34746, 'grapples'),\n",
              " (76152, 'summerize'),\n",
              " (30603, 'freelance'),\n",
              " (52283, \"andrea's\"),\n",
              " (52284, '\\x91very'),\n",
              " (45879, 'coolidge'),\n",
              " (18518, 'mache'),\n",
              " (52285, 'balled'),\n",
              " (40937, 'grappled'),\n",
              " (18519, 'macha'),\n",
              " (21921, 'underlining'),\n",
              " (5623, 'macho'),\n",
              " (19507, 'oversight'),\n",
              " (25257, 'machi'),\n",
              " (11311, 'verbally'),\n",
              " (21922, 'tenacious'),\n",
              " (40938, 'windshields'),\n",
              " (18557, 'paychecks'),\n",
              " (3396, 'jerk'),\n",
              " (11931, \"good'\"),\n",
              " (34748, 'prancer'),\n",
              " (21923, 'prances'),\n",
              " (52286, 'olympus'),\n",
              " (21924, 'lark'),\n",
              " (10785, 'embark'),\n",
              " (7365, 'gloomy'),\n",
              " (52287, 'jehaan'),\n",
              " (52288, 'turaqui'),\n",
              " (20607, \"child'\"),\n",
              " (2894, 'locked'),\n",
              " (52289, 'pranced'),\n",
              " (2588, 'exact'),\n",
              " (52290, 'unattuned'),\n",
              " (783, 'minute'),\n",
              " (16118, 'skewed'),\n",
              " (40940, 'hodgins'),\n",
              " (34749, 'skewer'),\n",
              " (52291, 'think\\x85'),\n",
              " (38765, 'rosenstein'),\n",
              " (52292, 'helmit'),\n",
              " (34750, 'wrestlemanias'),\n",
              " (16826, 'hindered'),\n",
              " (30604, \"martha's\"),\n",
              " (52293, 'cheree'),\n",
              " (52294, \"pluckin'\"),\n",
              " (40941, 'ogles'),\n",
              " (11932, 'heavyweight'),\n",
              " (82190, 'aada'),\n",
              " (11312, 'chopping'),\n",
              " (61534, 'strongboy'),\n",
              " (41342, 'hegemonic'),\n",
              " (40942, 'adorns'),\n",
              " (41346, 'xxth'),\n",
              " (34751, 'nobuhiro'),\n",
              " (52298, 'capitães'),\n",
              " (52299, 'kavogianni'),\n",
              " (13422, 'antwerp'),\n",
              " (6538, 'celebrated'),\n",
              " (52300, 'roarke'),\n",
              " (40943, 'baggins'),\n",
              " (31270, 'cheeseburgers'),\n",
              " (52301, 'matras'),\n",
              " (52302, \"nineties'\"),\n",
              " (52303, \"'craig'\"),\n",
              " (12999, 'celebrates'),\n",
              " (3383, 'unintentionally'),\n",
              " (14362, 'drafted'),\n",
              " (52304, 'climby'),\n",
              " (52305, '303'),\n",
              " (18520, 'oldies'),\n",
              " (9096, 'climbs'),\n",
              " (9655, 'honour'),\n",
              " (34752, 'plucking'),\n",
              " (30074, '305'),\n",
              " (5514, 'address'),\n",
              " (40944, 'menjou'),\n",
              " (42592, \"'freak'\"),\n",
              " (19508, 'dwindling'),\n",
              " (9458, 'benson'),\n",
              " (52307, 'white’s'),\n",
              " (40945, 'shamelessness'),\n",
              " (21925, 'impacted'),\n",
              " (52308, 'upatz'),\n",
              " (3840, 'cusack'),\n",
              " (37567, \"flavia's\"),\n",
              " (52309, 'effette'),\n",
              " (34753, 'influx'),\n",
              " (52310, 'boooooooo'),\n",
              " (52311, 'dimitrova'),\n",
              " (13423, 'houseman'),\n",
              " (25259, 'bigas'),\n",
              " (52312, 'boylen'),\n",
              " (52313, 'phillipenes'),\n",
              " (40946, 'fakery'),\n",
              " (27658, \"grandpa's\"),\n",
              " (27659, 'darnell'),\n",
              " (19509, 'undergone'),\n",
              " (52315, 'handbags'),\n",
              " (21926, 'perished'),\n",
              " (37778, 'pooped'),\n",
              " (27660, 'vigour'),\n",
              " (3627, 'opposed'),\n",
              " (52316, 'etude'),\n",
              " (11799, \"caine's\"),\n",
              " (52317, 'doozers'),\n",
              " (34754, 'photojournals'),\n",
              " (52318, 'perishes'),\n",
              " (34755, 'constrains'),\n",
              " (40948, 'migenes'),\n",
              " (30605, 'consoled'),\n",
              " (16827, 'alastair'),\n",
              " (52319, 'wvs'),\n",
              " (52320, 'ooooooh'),\n",
              " (34756, 'approving'),\n",
              " (40949, 'consoles'),\n",
              " (52064, 'disparagement'),\n",
              " (52322, 'futureistic'),\n",
              " (52323, 'rebounding'),\n",
              " (52324, \"'date\"),\n",
              " (52325, 'gregoire'),\n",
              " (21927, 'rutherford'),\n",
              " (34757, 'americanised'),\n",
              " (82196, 'novikov'),\n",
              " (1042, 'following'),\n",
              " (34758, 'munroe'),\n",
              " (52326, \"morita'\"),\n",
              " (52327, 'christenssen'),\n",
              " (23106, 'oatmeal'),\n",
              " (25260, 'fossey'),\n",
              " (40950, 'livered'),\n",
              " (13000, 'listens'),\n",
              " (76164, \"'marci\"),\n",
              " (52330, \"otis's\"),\n",
              " (23387, 'thanking'),\n",
              " (16019, 'maude'),\n",
              " (34759, 'extensions'),\n",
              " (52332, 'ameteurish'),\n",
              " (52333, \"commender's\"),\n",
              " (27661, 'agricultural'),\n",
              " (4518, 'convincingly'),\n",
              " (17639, 'fueled'),\n",
              " (54014, 'mahattan'),\n",
              " (40952, \"paris's\"),\n",
              " (52336, 'vulkan'),\n",
              " (52337, 'stapes'),\n",
              " (52338, 'odysessy'),\n",
              " (12259, 'harmon'),\n",
              " (4252, 'surfing'),\n",
              " (23494, 'halloran'),\n",
              " (49580, 'unbelieveably'),\n",
              " (52339, \"'offed'\"),\n",
              " (30607, 'quadrant'),\n",
              " (19510, 'inhabiting'),\n",
              " (34760, 'nebbish'),\n",
              " (40953, 'forebears'),\n",
              " (34761, 'skirmish'),\n",
              " (52340, 'ocassionally'),\n",
              " (52341, \"'resist\"),\n",
              " (21928, 'impactful'),\n",
              " (52342, 'spicier'),\n",
              " (40954, 'touristy'),\n",
              " (52343, \"'football'\"),\n",
              " (40955, 'webpage'),\n",
              " (52345, 'exurbia'),\n",
              " (52346, 'jucier'),\n",
              " (14901, 'professors'),\n",
              " (34762, 'structuring'),\n",
              " (30608, 'jig'),\n",
              " (40956, 'overlord'),\n",
              " (25261, 'disconnect'),\n",
              " (82201, 'sniffle'),\n",
              " (40957, 'slimeball'),\n",
              " (40958, 'jia'),\n",
              " (16828, 'milked'),\n",
              " (40959, 'banjoes'),\n",
              " (1237, 'jim'),\n",
              " (52348, 'workforces'),\n",
              " (52349, 'jip'),\n",
              " (52350, 'rotweiller'),\n",
              " (34763, 'mundaneness'),\n",
              " (52351, \"'ninja'\"),\n",
              " (11040, \"dead'\"),\n",
              " (40960, \"cipriani's\"),\n",
              " (20608, 'modestly'),\n",
              " (52352, \"professor'\"),\n",
              " (40961, 'shacked'),\n",
              " (34764, 'bashful'),\n",
              " (23388, 'sorter'),\n",
              " (16120, 'overpowering'),\n",
              " (18521, 'workmanlike'),\n",
              " (27662, 'henpecked'),\n",
              " (18522, 'sorted'),\n",
              " (52354, \"jōb's\"),\n",
              " (52355, \"'always\"),\n",
              " (34765, \"'baptists\"),\n",
              " (52356, 'dreamcatchers'),\n",
              " (52357, \"'silence'\"),\n",
              " (21929, 'hickory'),\n",
              " (52358, 'fun\\x97yet'),\n",
              " (52359, 'breakumentary'),\n",
              " (15496, 'didn'),\n",
              " (52360, 'didi'),\n",
              " (52361, 'pealing'),\n",
              " (40962, 'dispite'),\n",
              " (25262, \"italy's\"),\n",
              " (21930, 'instability'),\n",
              " (6539, 'quarter'),\n",
              " (12608, 'quartet'),\n",
              " (52362, 'padmé'),\n",
              " (52363, \"'bleedmedry\"),\n",
              " (52364, 'pahalniuk'),\n",
              " (52365, 'honduras'),\n",
              " (10786, 'bursting'),\n",
              " (41465, \"pablo's\"),\n",
              " (52367, 'irremediably'),\n",
              " (40963, 'presages'),\n",
              " (57832, 'bowlegged'),\n",
              " (65183, 'dalip'),\n",
              " (6260, 'entering'),\n",
              " (76172, 'newsradio'),\n",
              " (54150, 'presaged'),\n",
              " (27663, \"giallo's\"),\n",
              " (40964, 'bouyant'),\n",
              " (52368, 'amerterish'),\n",
              " (18523, 'rajni'),\n",
              " (30610, 'leeves'),\n",
              " (34767, 'macauley'),\n",
              " (612, 'seriously'),\n",
              " (52369, 'sugercoma'),\n",
              " (52370, 'grimstead'),\n",
              " (52371, \"'fairy'\"),\n",
              " (30611, 'zenda'),\n",
              " (52372, \"'twins'\"),\n",
              " (17640, 'realisation'),\n",
              " (27664, 'highsmith'),\n",
              " (7817, 'raunchy'),\n",
              " (40965, 'incentives'),\n",
              " (52374, 'flatson'),\n",
              " (35097, 'snooker'),\n",
              " (16829, 'crazies'),\n",
              " (14902, 'crazier'),\n",
              " (7094, 'grandma'),\n",
              " (52375, 'napunsaktha'),\n",
              " (30612, 'workmanship'),\n",
              " (52376, 'reisner'),\n",
              " (61306, \"sanford's\"),\n",
              " (52377, '\\x91doña'),\n",
              " (6108, 'modest'),\n",
              " (19153, \"everything's\"),\n",
              " (40966, 'hamer'),\n",
              " (52379, \"couldn't'\"),\n",
              " (13001, 'quibble'),\n",
              " (52380, 'socking'),\n",
              " (21931, 'tingler'),\n",
              " (52381, 'gutman'),\n",
              " (40967, 'lachlan'),\n",
              " (52382, 'tableaus'),\n",
              " (52383, 'headbanger'),\n",
              " (2847, 'spoken'),\n",
              " (34768, 'cerebrally'),\n",
              " (23490, \"'road\"),\n",
              " (21932, 'tableaux'),\n",
              " (40968, \"proust's\"),\n",
              " (40969, 'periodical'),\n",
              " (52385, \"shoveller's\"),\n",
              " (25263, 'tamara'),\n",
              " (17641, 'affords'),\n",
              " (3249, 'concert'),\n",
              " (87955, \"yara's\"),\n",
              " (52386, 'someome'),\n",
              " (8424, 'lingering'),\n",
              " (41511, \"abraham's\"),\n",
              " (34769, 'beesley'),\n",
              " (34770, 'cherbourg'),\n",
              " (28624, 'kagan'),\n",
              " (9097, 'snatch'),\n",
              " (9260, \"miyazaki's\"),\n",
              " (25264, 'absorbs'),\n",
              " (40970, \"koltai's\"),\n",
              " (64027, 'tingled'),\n",
              " (19511, 'crossroads'),\n",
              " (16121, 'rehab'),\n",
              " (52389, 'falworth'),\n",
              " (52390, 'sequals'),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qjjy69irfzB"
      },
      "source": [
        "###decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]]) \n",
        "#' '.join이거는 리스트에 대해서 ' '를 하면서 합쳐주겠다는 것이다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHivpDt0sH1i",
        "outputId": "bf7e4ecb-d355-4aa6-f9ab-2a50a341ff42"
      },
      "source": [
        "###train_data[0][:10]  \n",
        "#여기에 있는 10개의 숫자들이 아래 보는 것 처럼 단어들에 10개의 숫자들이 매핑이되는 것을 볼 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9CW7NOxrwed",
        "outputId": "4a2ec26e-2025-495b-973b-0c274774e4d5"
      },
      "source": [
        "###[reverse_word_index.get(i - 3, '?') for i in train_data[0][:10]]\n",
        "#train_data[0][:10]이거는 첫번째 리뷰의 10개의 단어들을 본다는 말이다.\n",
        "#i - 3이거를 사용하는 이유는 num_words=10000을 사용하는 코딩과정에서 사용되는 메소드가 10000이 넘어가는 단어들을 0,1,2쪽에다가 배치시키는 것 같다.\n",
        "#그래서 그런애들은 의미가 없으니깐 그런애들이 걸린 경우에는 '?'표로 갈 수 있게끔하고 여기서는 인덱스를 3부터 받아 오는 걸로 해준다.\n",
        "#그리고 만약 i가 0,1,2라서 i-3해서 -3,-2,-1이 나오게 되고 이걸 reverse_word_index에 넣게 되면 값이 없기 때문에, '?'표를 집어 넣겠다는 것이다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['?',\n",
              " 'this',\n",
              " 'film',\n",
              " 'was',\n",
              " 'just',\n",
              " 'brilliant',\n",
              " 'casting',\n",
              " 'location',\n",
              " 'scenery',\n",
              " 'story']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WzW_us_AuOJd",
        "outputId": "66d59f03-57a4-4e71-e3b6-d5da2543c7a8"
      },
      "source": [
        "###' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0][:10]]) #' '.join을 하게 되면 스페이스로 조절을 하겠다는 것이다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'? this film was just brilliant casting location scenery story'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W1x1OjYB8fk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e8cb90-61b1-4058-b3ec-89571c35a088"
      },
      "source": [
        "# word_index is a dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()  #Dcoding하는 것에 대해서 나오고 있다.\n",
        "# We reverse it, mapping integer indices to words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# We decode the review; note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDYOGYOlB8fk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "b4e5c4fb-4cb4-46d2-8276-bcc2048e6db4"
      },
      "source": [
        "decoded_review   #train data에 있는 것을 전체 decoded하게 되면 아래와 같은 문장이 나오게 된다. 그래서 아래 있는게 review data가 되는 것이다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0YohWrVB8fk"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "\n",
        "We cannot feed lists of integers into a neural network. We have to turn our lists into tensors. There are two ways we could do that:\n",
        "\n",
        "* We could pad our lists so that they all have the same length, and turn them into an integer tensor of shape `(samples, word_indices)`, \n",
        "then use as first layer in our network a layer capable of handling such integer tensors (the `Embedding` layer, which we will cover in \n",
        "detail later in the book).\n",
        "* We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence \n",
        "`[3, 5]` into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as \n",
        "first layer in our network a `Dense` layer, capable of handling floating point vector data.\n",
        "\n",
        "We will go with the latter solution. Let's vectorize our data, which we will do manually for maximum clarity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7yKLZhk7ZFR",
        "outputId": "e862866a-5535-4056-c81b-2bd8d16ccc83"
      },
      "source": [
        "for i, sequence in enumerate(['Andy','Tom','Max']):\n",
        "  print(i, sequence)   #enumerate를 하게 되면, 리스트를 sequence에 넣게 되고 그리고 그 리스트에 숫자가 부여된다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Andy\n",
            "1 Tom\n",
            "2 Max\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GSe84TP7znF",
        "outputId": "ed983d74-5c97-44d1-9828-5a73c6beb677"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.zeros((2,3))  #zeros는 2,3을 넣어 주었을 때, 0.을 2행 3열로 만들어 준다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeqqbiWKB8fk"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):   #vectorize_sequences해서 이 함수를 정의 한다음에,그 함수를 돌려서 x_train과 x_test를 넣어 주었다.\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHtejoyKB8fl"
      },
      "source": [
        "Here's what our samples look like now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhNv2ZrTB8fl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ff3fdf-0064-4053-e561-f78351b8ea33"
      },
      "source": [
        "x_train[0]  #x_train에 대해 살펴보면 이렇게 되는 것을 알 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvZ-8vkiB8fl"
      },
      "source": [
        "We should also vectorize our labels, which is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpiZs76c8-Lr",
        "outputId": "9d542787-14ec-4d4f-fda6-c855599956e5"
      },
      "source": [
        "train_labels[:5]  #train_labels하게 되면 integer값으로 되어 있는 것을 알 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTP76JjQB8fl"
      },
      "source": [
        "# Our vectorized labels\n",
        "y_train = np.asarray(train_labels).astype('float32')  #train_labels을 np.asarray로 바꾸면서 타입도 float32로 바꾸겠다는 것이다.\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAh6IaSA9f9k",
        "outputId": "c44a2bea-02f4-4560-83bb-9c4d050ddd56"
      },
      "source": [
        "y_train[:3]  #결과 array이 타입에서 float으로 가게 된다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02uFiqgOB8fl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04505023-2c4e-418c-9bfd-39264558d02a"
      },
      "source": [
        "train_labels[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG7VZpjEB8fm"
      },
      "source": [
        "Now our data is ready to be fed into a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc7pPmB8B8fm"
      },
      "source": [
        "## Building our network\n",
        "\n",
        "\n",
        "Our input data is simply vectors, and our labels are scalars (1s and 0s): this is the easiest setup you will ever encounter. A type of \n",
        "network that performs well on such a problem would be a simple stack of fully-connected (`Dense`) layers with `relu` activations: `Dense(16, \n",
        "activation='relu')`\n",
        "\n",
        "The argument being passed to each `Dense` layer (16) is the number of \"hidden units\" of the layer. What's a hidden unit? It's a dimension \n",
        "in the representation space of the layer. You may remember from the previous chapter that each such `Dense` layer with a `relu` activation implements \n",
        "the following chain of tensor operations:\n",
        "\n",
        "`output = relu(dot(W, input) + b)`\n",
        "\n",
        "Having 16 hidden units means that the weight matrix `W` will have shape `(input_dimension, 16)`, i.e. the dot product with `W` will project the \n",
        "input data onto a 16-dimensional representation space (and then we would add the bias vector `b` and apply the `relu` operation). You can \n",
        "intuitively understand the dimensionality of your representation space as \"how much freedom you are allowing the network to have when \n",
        "learning internal representations\". Having more hidden units (a higher-dimensional representation space) allows your network to learn more \n",
        "complex representations, but it makes your network more computationally expensive and may lead to learning unwanted patterns (patterns that \n",
        "will improve performance on the training data but not on the test data).\n",
        "\n",
        "There are two key architecture decisions to be made about such stack of dense layers:\n",
        "\n",
        "* How many layers to use.\n",
        "* How many \"hidden units\" to chose for each layer.\n",
        "\n",
        "In the next chapter, you will learn formal principles to guide you in making these choices. \n",
        "For the time being, you will have to trust us with the following architecture choice: \n",
        "two intermediate layers with 16 hidden units each, \n",
        "and a third layer which will output the scalar prediction regarding the sentiment of the current review. \n",
        "The intermediate layers will use `relu` as their \"activation function\", \n",
        "and the final layer will use a sigmoid activation so as to output a probability \n",
        "(a score between 0 and 1, indicating how likely the sample is to have the target \"1\", i.e. how likely the review is to be positive). \n",
        "A `relu` (rectified linear unit) is a function meant to zero-out negative values, \n",
        "while a sigmoid \"squashes\" arbitrary values into the `[0, 1]` interval, thus outputting something that can be interpreted as a probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "283ptmnfB8fm"
      },
      "source": [
        "Here's what our network looks like:\n",
        "\n",
        "![3-layer network](https://s3.amazonaws.com/book.keras.io/img/ch3/3_layer_network.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6Z04wgO-q98"
      },
      "source": [
        "**2021.3.16 수업 중 필기**\n",
        "- 여기에서는 dense를 사용해서 input =  10000 데이터를 16으로 만들고 그리고 16으로 만든다음에 1로 만들고 나서 그걸로 movie review를 판별해 주겠다는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlujytmMB8fm"
      },
      "source": [
        "And here's the Keras implementation, very similar to the MNIST example you saw previously:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOCez5l4B8fm"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = models.Sequential()   #여기에서는 keras에 Sequenrial이라는 것을 사용했고, models layer을 만들게 되면 그 다음에는 하나씩 하니씩 붙여주게 된다.\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))   #model.add를 사용해서 처음 input_shape가 처음에 10000차원이 들어오고  그걸 16차원으로 만들고\n",
        "model.add(layers.Dense(16, activation='relu'))  #그 다음인 여기에서도 dense layers에 16차원을 달아주고,\n",
        "model.add(layers.Dense(1, activation='sigmoid')) #그 다음 여기에서는 1개 짜리인 denses layers를 달아주는데, activation은 sigmoid로 해서 만들어 준다. sigmoid는 0에서 1사이의 값이 나오게 만들어 준다.\n",
        "#그래서 sigmoid가 높으면 1, 낮으면 0으로 만들어 준다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro58ybKbB8fn"
      },
      "source": [
        "\n",
        "Lastly, we need to pick a loss function and an optimizer. Since we are facing a binary classification problem and the output of our network \n",
        "is a probability (we end our network with a single-unit layer with a sigmoid activation), is it best to use the `binary_crossentropy` loss. \n",
        "It isn't the only viable choice: you could use, for instance, `mean_squared_error`. But crossentropy is usually the best choice when you \n",
        "are dealing with models that output probabilities. Crossentropy is a quantity from the field of Information Theory, that measures the \"distance\" \n",
        "between probability distributions, or in our case, between the ground-truth distribution and our predictions.\n",
        "\n",
        "Here's the step where we configure our model with the `rmsprop` optimizer and the `binary_crossentropy` loss function. Note that we will \n",
        "also monitor accuracy during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3ZbMedFB8fn"
      },
      "source": [
        "model.compile(optimizer='rmsprop',  #compile할 때 optimizer는 rmprop를 사용하고\n",
        "              loss='binary_crossentropy', #우리가 가지고 있는게 binary이기 때문에, loss를 binary_crossentropy를 사용해준다.\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzfwX7X2B8fo"
      },
      "source": [
        "We are passing our optimizer, loss function and metrics as strings, which is possible because `rmsprop`, `binary_crossentropy` and \n",
        "`accuracy` are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer, or pass a custom loss \n",
        "function or metric function. This former can be done by passing an optimizer class instance as the `optimizer` argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW6JEMzNB8fo"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001), #모형에서 runing weights를 세부적으로 만들어 주는 방법도 있다. \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eiyBAr8AfBl"
      },
      "source": [
        "-  여기에서는 위에 있는 코드를 세부적으로 표현 해 준 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDj7kKviB8fo"
      },
      "source": [
        "The latter can be done by passing function objects as the `loss` or `metrics` arguments:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnBvjEerB8fp"
      },
      "source": [
        "## Validating our approach\n",
        "\n",
        "In order to monitor during training the accuracy of the model on data that it has never seen before, we will create a \"validation set\" by \n",
        "setting apart 10,000 samples from the original training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EwmZMArB8fp"
      },
      "source": [
        "x_val = x_train[:10000]   #validation에서는 첫 번째를 10000개로 하고, x_val = x_train[:10000]   \n",
        "partial_x_train = x_train[10000:]   #그리고 partial_x_train을 10001개부터 해서 쓰겠다는 것이다.\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "z1UNdsy8B8fp"
      },
      "source": [
        "We will now train our model for 20 epochs (20 iterations over all samples in the `x_train` and `y_train` tensors), in mini-batches of 512 \n",
        "samples. At this same time we will monitor loss and accuracy on the 10,000 samples that we set apart. This is done by passing the \n",
        "validation data as the `validation_data` argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTMBe0ZgB8fp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ee262c-81c6-410c-a462-9da404c615da"
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30/30 [==============================] - 3s 80ms/step - loss: 0.5706 - accuracy: 0.7250 - val_loss: 0.3764 - val_accuracy: 0.8726\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.3248 - accuracy: 0.8948 - val_loss: 0.3155 - val_accuracy: 0.8745\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.2328 - accuracy: 0.9285 - val_loss: 0.3127 - val_accuracy: 0.8707\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.1833 - accuracy: 0.9412 - val_loss: 0.2906 - val_accuracy: 0.8810\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.1560 - accuracy: 0.9491 - val_loss: 0.2805 - val_accuracy: 0.8867\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.1159 - accuracy: 0.9673 - val_loss: 0.3060 - val_accuracy: 0.8800\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.0972 - accuracy: 0.9727 - val_loss: 0.3095 - val_accuracy: 0.8812\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.0807 - accuracy: 0.9801 - val_loss: 0.3204 - val_accuracy: 0.8807\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0674 - accuracy: 0.9838 - val_loss: 0.3404 - val_accuracy: 0.8817\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.0540 - accuracy: 0.9884 - val_loss: 0.4242 - val_accuracy: 0.8672\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.0464 - accuracy: 0.9898 - val_loss: 0.3862 - val_accuracy: 0.8745\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0400 - accuracy: 0.9921 - val_loss: 0.4183 - val_accuracy: 0.8767\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0317 - accuracy: 0.9939 - val_loss: 0.4450 - val_accuracy: 0.8697\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0247 - accuracy: 0.9962 - val_loss: 0.4795 - val_accuracy: 0.8725\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.0185 - accuracy: 0.9981 - val_loss: 0.5028 - val_accuracy: 0.8686\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.0142 - accuracy: 0.9985 - val_loss: 0.5353 - val_accuracy: 0.8707\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.0105 - accuracy: 0.9993 - val_loss: 0.5701 - val_accuracy: 0.8658\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0109 - accuracy: 0.9987 - val_loss: 0.6015 - val_accuracy: 0.8673\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0070 - accuracy: 0.9993 - val_loss: 0.6326 - val_accuracy: 0.8668\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.0043 - accuracy: 0.9998 - val_loss: 0.6880 - val_accuracy: 0.8598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvGuHFtIGOSf"
      },
      "source": [
        "- training된 결과들이 history에 저장이 된다. 그리고 history로 모델 피팅과정을 저장할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgRLo9UjB8fp"
      },
      "source": [
        "On CPU, this will take less than two seconds per epoch -- training is over in 20 seconds. At the end of every epoch, there is a slight pause \n",
        "as the model computes its loss and accuracy on the 10,000 samples of the validation data.\n",
        "\n",
        "Note that the call to `model.fit()` returns a `History` object. This object has a member `history`, which is a dictionary containing data \n",
        "about everything that happened during training. Let's take a look at it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTO0Z254B8fq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64880f77-6c46-4dec-94a8-5319949cab5d"
      },
      "source": [
        "history_dict = history.history   #history.history로 해서 저장을 하고,\n",
        "history_dict.keys()   #history_dic에서 keys들을 꺼내 보았을 때 아래와 같은 결과가 나오게 된다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOUk_wguB8fq"
      },
      "source": [
        "It contains 4 entries: one per metric that was being monitored, during training and during validation. Let's use Matplotlib to plot the \n",
        "training and validation loss side by side, as well as the training and validation accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6cUA31DB8fq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "64ee0e5e-f4ba-403f-e11a-c390c22e081f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdttJeEHHH_k"
      },
      "source": [
        "- 피큐어 같은 경우에는 Training loss가 진행되면서 validation loss가 줄어들다가 점 점 높아지는 것을 볼 수 있고, Training loss는 점점 줄어드는 것을 볼 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP5-Ll9dB8fq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d37dd765-d62a-47b4-fe46-5281641b427d"
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toN_ftVdHxSb"
      },
      "source": [
        "- accuracy를 돌려보면, Training accuracy는 점점 1에 가깝게 과적합되어 가는데, validtion accuracy는 점점 떨어지는 것을 볼 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DnA59BJB8fr"
      },
      "source": [
        "\n",
        "The dots are the training loss and accuracy, while the solid lines are the validation loss and accuracy. Note that your own results may vary \n",
        "slightly due to a different random initialization of your network.\n",
        "\n",
        "As you can see, the training loss decreases with every epoch and the training accuracy increases with every epoch. That's what you would \n",
        "expect when running gradient descent optimization -- the quantity you are trying to minimize should get lower with every iteration. But that \n",
        "isn't the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we were warning \n",
        "against earlier: a model that performs better on the training data isn't necessarily a model that will do better on data it has never seen \n",
        "before. In precise terms, what you are seeing is \"overfitting\": after the second epoch, we are over-optimizing on the training data, and we \n",
        "ended up learning representations that are specific to the training data and do not generalize to data outside of the training set.\n",
        "\n",
        "In this case, to prevent overfitting, we could simply stop training after three epochs. In general, there is a range of techniques you can \n",
        "leverage to mitigate overfitting, which we will cover in the next chapter.\n",
        "\n",
        "Let's train a new network from scratch for four epochs, then evaluate it on our test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCPRtoCJB8fr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3cc9655-a145-4b3a-b5f0-72f6856856b1"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "49/49 [==============================] - 2s 28ms/step - loss: 0.5519 - accuracy: 0.7542\n",
            "Epoch 2/4\n",
            "49/49 [==============================] - 1s 28ms/step - loss: 0.2670 - accuracy: 0.9112\n",
            "Epoch 3/4\n",
            "49/49 [==============================] - 1s 28ms/step - loss: 0.2006 - accuracy: 0.9315\n",
            "Epoch 4/4\n",
            "49/49 [==============================] - 1s 28ms/step - loss: 0.1683 - accuracy: 0.9421\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.2913 - accuracy: 0.8855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St-b7ZrXIJoO"
      },
      "source": [
        "- 여기서는 epochs를 4개 까지만 해서 한 것이다. epochs를 4개만 해주는 이유는 위에 있는 코드들에서 varidation이 어느정도에서 높아지다가 계속 높아지기 때문에 여기에서는 epochs를 4개 까지만 해서 varidation이 처음 높아지는 부분을 보겠다는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOUQ0hRjB8fr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdf6ce5-d5c4-4107-8657-8a50d021ff6c"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.29126131534576416, 0.8855199813842773]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyjcPD-1Izq2"
      },
      "source": [
        "- x_tes와 y_test를 evaluate해줘서 results로 결과를 보는 것이다.\n",
        "- results결과 우리가 적합한 모형은 87%의 성능을 보인다는 것을 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4_Nb0v-B8fs"
      },
      "source": [
        "Our fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, one should be able to get close to 95%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6bUxyeZB8fs"
      },
      "source": [
        "## Using a trained network to generate predictions on new data\n",
        "\n",
        "After having trained a network, you will want to use it in a practical setting. You can generate the likelihood of reviews being positive \n",
        "by using the `predict` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwCMKUQSB8fs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7bc5a21-d286-4e39-c8c5-9e0729a78883"
      },
      "source": [
        "model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.13477811],\n",
              "       [0.9998704 ],\n",
              "       [0.5271985 ],\n",
              "       ...,\n",
              "       [0.12590885],\n",
              "       [0.04763117],\n",
              "       [0.5724081 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6w3x29iJRhg"
      },
      "source": [
        "- .predict를 하게 되면 x_test의 predict의 값을 볼 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "744NW-OqB8fs"
      },
      "source": [
        "As you can see, the network is very confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.6, 0.4). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJvE4olEB8fs"
      },
      "source": [
        "## Further experiments\n",
        "\n",
        "\n",
        "* We were using 2 hidden layers. Try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.\n",
        "* Try to use layers with more hidden units or less hidden units: 32 units, 64 units...\n",
        "* Try to use the `mse` loss function instead of `binary_crossentropy`.\n",
        "* Try to use the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`.\n",
        "\n",
        "These experiments will help convince you that the architecture choices we have made are all fairly reasonable, although they can still be \n",
        "improved!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv_0yZfGB8fs"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "\n",
        "Here's what you should take away from this example:\n",
        "\n",
        "* There's usually quite a bit of preprocessing you need to do on your raw data in order to be able to feed it -- as tensors -- into a neural \n",
        "network. In the case of sequences of words, they can be encoded as binary vectors -- but there are other encoding options too.\n",
        "* Stacks of `Dense` layers with `relu` activations can solve a wide range of problems (including sentiment classification), and you will \n",
        "likely use them frequently.\n",
        "* In a binary classification problem (two output classes), your network should end with a `Dense` layer with 1 unit and a `sigmoid` activation, \n",
        "i.e. the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
        "* With such a scalar sigmoid output, on a binary classification problem, the loss function you should use is `binary_crossentropy`.\n",
        "* The `rmsprop` optimizer is generally a good enough choice of optimizer, whatever your problem. That's one less thing for you to worry \n",
        "about.\n",
        "* As they get better on their training data, neural networks eventually start _overfitting_ and end up obtaining increasingly worse results on data \n",
        "never-seen-before. Make sure to always monitor performance on data that is outside of the training set.\n"
      ]
    }
  ]
}